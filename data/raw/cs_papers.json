[
  {
    "id": "2508.00788v1",
    "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models",
    "abstract": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
    "clean_title": "do they understand them? an updated evaluation on nonbinary pronoun handling in large language models",
    "clean_abstract": "large language models llms are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible ai. prior work, such as the misgendered benchmark, revealed significant limitations in earlier llms handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. in this study, we introduce misgendered, an extended and updated benchmark for evaluating llms pronoun fidelity. we benchmark five representative llms, gpt-4o, claude 4, deepseek-v3, qwen turbo, and qwen2.5, across zero-shot, few-shot, and gender identity inference. our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. however, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. we discuss implications, model-specific observations, and avenues for future inclusive ai research.",
    "authors": [
      "Xushuo Tang",
      "Yi Ding",
      "Zhengyi Yang",
      "Yin Chen",
      "Yongrui Gu",
      "Wenke Yang",
      "Mingchen Ju",
      "Xin Cao",
      "Yongfei Liu",
      "Wenjie Zhang"
    ],
    "author_count": 10,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "pronoun",
      "models",
      "llms",
      "updated",
      "handling",
      "large",
      "language",
      "especially",
      "gender-neutral",
      "remains"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00788v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00788v1"
  },
  {
    "id": "2508.00784v1",
    "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics",
    "abstract": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.",
    "clean_title": "unraveling hidden representations: a multi-modal layer analysis for better synthetic content forensics",
    "clean_abstract": "generative models achieve remarkable results in multiple data domains, including images and texts, among other examples. unfortunately, malicious users exploit synthetic media for spreading misinformation and disseminating deepfakes. consequently, the need for robust and stable fake detectors is pressing, especially when new generative models appear everyday. while the majority of existing work train classifiers that discriminate between real and fake information, such tools typically generalize only within the same family of generators and data modalities, yielding poor results on other generative classes and data domains. towards a universal classifier, we propose the use of large pre-trained multi-modal models for the detection of generative content. effectively, we show that the latent code of these models naturally captures information discriminating real from fake. building on this observation, we demonstrate that linear classifiers trained on these features can achieve state-of-the-art results across various modalities, while remaining computationally efficient, fast to train, and effective even in few-shot settings. our work primarily focuses on fake detection in audio and images, achieving performance that surpasses or matches that of strong baseline methods.",
    "authors": [
      "Tom Or",
      "Omri Azencot"
    ],
    "author_count": 2,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "keywords": [
      "generative",
      "models",
      "results",
      "data",
      "fake",
      "multi-modal",
      "synthetic",
      "achieve",
      "other",
      "while"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00784v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00784v1"
  },
  {
    "id": "2508.00782v1",
    "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation",
    "abstract": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.",
    "clean_title": "spa2v: harnessing spatial auditory cues for audio-driven spatially-aware video generation",
    "clean_abstract": "audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. however, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. in contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. this useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. as prior methods largely ignore this factor, we present spa2v, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. spa2v decomposes the generation process into two stages: 1 audio-guided video planning: we meticulously adapt a state-of-the-art mllm for a novel task of harnessing spatial and semantic cues from input audio to construct video scene layouts vsls. this serves as an intermediate representation to bridge the gap between the audio and video modalities. 2 layout-grounded video generation: we develop an efficient and effective approach to seamlessly integrate vsls as conditional guidance into pre-trained diffusion models, enabling vsl-grounded video generation in a training-free manner. extensive experiments demonstrate that spa2v excels in generating realistic videos with semantic and spatial alignment to the input audios.",
    "authors": [
      "Kien T. Pham",
      "Yingqing He",
      "Yazhou Xing",
      "Qifeng Chen",
      "Long Chen"
    ],
    "author_count": 5,
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.GR",
    "keywords": [
      "spatial",
      "video",
      "semantic",
      "generation",
      "videos",
      "from",
      "auditory",
      "cues",
      "input",
      "audio"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00782v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00782v1"
  },
  {
    "id": "2508.00766v1",
    "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation",
    "abstract": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.",
    "clean_title": "sample-aware test-time adaptation for medical image-to-image translation",
    "clean_abstract": "image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. however, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. to address this limitation, we propose a novel test-time adaptation tta framework that dynamically adjusts the translation process based on the characteristics of each test sample. our method introduces a reconstruction module to quantify the domain shift and a dynamic adaptation block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. we evaluate our approach on two medical image-to-image translation tasks: low-dose ct denoising and t1 to t2 mri translation, showing consistent improvements over both the baseline translation model without tta and prior tta methods. our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. the code is available at: https:github.comcosbidevsample-aware_tta.",
    "authors": [
      "Irene Iele",
      "Francesco Di Feola",
      "Valerio Guarrasi",
      "Paolo Soda"
    ],
    "author_count": 4,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "translation",
      "adaptation",
      "medical",
      "image-to-image",
      "without",
      "tta",
      "model",
      "test-time",
      "denoising",
      "limitations"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00766v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00766v1"
  },
  {
    "id": "2508.00760v1",
    "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations",
    "abstract": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.",
    "clean_title": "mmbert: scaled mixture-of-experts multimodal bert for robust chinese hate speech detection under cloaking perturbations",
    "clean_abstract": "hate speech detection on chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. although large language models llms have recently improved hate speech detection capabilities, the majority of existing work has concentrated on english datasets, with limited attention given to multimodal strategies in the chinese context. in this study, we propose mmbert, a novel bert-based multimodal framework that integrates textual, speech, and visual modalities through a mixture-of-experts moe architecture. to address the instability associated with directly integrating moe into bert-based models, we develop a progressive three-stage training paradigm. mmbert incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. empirical results in several chinese hate speech datasets show that mmbert significantly surpasses fine-tuned bert-based encoder models, fine-tuned llms, and llms utilizing in-context learning approaches.",
    "authors": [
      "Qiyao Xue",
      "Yuchen Dou",
      "Ryan Shi",
      "Xiang Lorraine Li",
      "Wei Gao"
    ],
    "author_count": 5,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "chinese",
      "hate",
      "speech",
      "detection",
      "multimodal",
      "bert-based",
      "mixture-of-experts",
      "cloaking",
      "llms",
      "moe"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00760v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00760v1"
  },
  {
    "id": "2508.00754v1",
    "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection",
    "abstract": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.",
    "clean_title": "a simple and effective method for uncertainty quantification and ood detection",
    "clean_abstract": "bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. by utilizing a single deterministic model, we can solve the above issue. we propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution ood detection. specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. by comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. experiments were conducted on a 2d synthetic dataset two moons and three spirals as well as an ood detection task cifar-10 vs. svhn. the results demonstrate that our method outperforms baseline models.",
    "authors": [
      "Yaxin Ma",
      "Benjamin Colburn",
      "Jose C. Principe"
    ],
    "author_count": 3,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "density",
      "uncertainty",
      "ood",
      "feature",
      "space",
      "effective",
      "detection",
      "distributional",
      "simple",
      "quantification"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00754v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00754v1"
  },
  {
    "id": "2508.00816v1",
    "title": "Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process",
    "abstract": "Solving Markov Decision Processes (MDPs) remains a central challenge in\nsequential decision-making, especially when dealing with large state spaces and\nlong-term optimization criteria. A key step in Bellman dynamic programming\nalgorithms is the policy evaluation, which becomes computationally demanding in\ninfinite-horizon settings such as average-reward or discounted-reward\nformulations. In the context of Markov chains, aggregation and disaggregation\ntechniques have for a long time been used to reduce complexity by exploiting\nstructural decompositions. In this work, we extend these principles to a\nstructured class of MDPs. We define the Single-Input Superstate Decomposable\nMarkov Decision Process (SISDMDP), which combines Chiu's single-input\ndecomposition with Robertazzi's single-cycle recurrence property. When a policy\ninduces this structure, the resulting transition graph can be decomposed into\ninteracting components with centralized recurrence. We develop an exact and\nefficient policy evaluation method based on this structure. This yields a\nscalable solution applicable to both average and discounted reward MDPs.",
    "clean_title": "efficient solving of large single input superstate decomposable markovian decision process",
    "clean_abstract": "solving markov decision processes mdps remains a central challenge in sequential decision-making, especially when dealing with large state spaces and long-term optimization criteria. a key step in bellman dynamic programming algorithms is the policy evaluation, which becomes computationally demanding in infinite-horizon settings such as average-reward or discounted-reward formulations. in the context of markov chains, aggregation and disaggregation techniques have for a long time been used to reduce complexity by exploiting structural decompositions. in this work, we extend these principles to a structured class of mdps. we define the single-input superstate decomposable markov decision process sisdmdp, which combines chius single-input decomposition with robertazzis single-cycle recurrence property. when a policy induces this structure, the resulting transition graph can be decomposed into interacting components with centralized recurrence. we develop an exact and efficient policy evaluation method based on this structure. this yields a scalable solution applicable to both average and discounted reward mdps.",
    "authors": [
      "Youssef Ait El Mahjoub",
      "Jean-Michel Fourneau",
      "Salma Alouah"
    ],
    "author_count": 3,
    "categories": [
      "math.OC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "math.OC",
    "keywords": [
      "decision",
      "markov",
      "policy",
      "efficient",
      "solving",
      "large",
      "superstate",
      "decomposable",
      "process",
      "when"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00816v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00816v1"
  },
  {
    "id": "2508.00806v1",
    "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management",
    "abstract": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.",
    "clean_title": "adacc: adaptive compression and activation checkpointing for llm memory management",
    "clean_abstract": "training large language models often employs recomputation to alleviate memory pressure, which can introduce up to 30 overhead in real-world scenarios. in this paper, we propose adacc, a novel memory management framework that combines adaptive compression and activation checkpointing to reduce the gpu memory footprint. it comprises three modules: 1 we design layer-specific compression algorithms that account for outliers in llm tensors, instead of directly quantizing floats from fp16 to int4, to ensure model accuracy. 2 we propose an optimal scheduling policy that employs milp to determine the best memory optimization for each tensor. 3 to accommodate changes in training tensors, we introduce an adaptive policy evolution mechanism that adjusts the policy during training to enhance throughput. experimental results show that adacc can accelerate the llm training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy to the baseline.",
    "authors": [
      "Ping Chen",
      "Zhuohong Deng",
      "Ping Li",
      "Shuibing He",
      "Hongzi Zhu",
      "Yi Zheng",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Minyi Guo"
    ],
    "author_count": 9,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "memory",
      "training",
      "adaptive",
      "compression",
      "llm",
      "policy",
      "activation",
      "checkpointing",
      "management",
      "employs"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00806v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00806v1"
  },
  {
    "id": "2508.00804v1",
    "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models",
    "abstract": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.",
    "clean_title": "online fine-tuning of carbon emission predictions using real-time recurrent learning for state space models",
    "clean_abstract": "this paper introduces a new approach for fine-tuning the predictions of structured state space models ssms at inference time using real-time recurrent learning. while ssms are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. our method enables online adaptation by continuously updating model parameters in response to incoming data. we evaluate our approach for linear-recurrent-unit ssms using a small carbon emission dataset collected from embedded automotive hardware. experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.",
    "authors": [
      "Julian Lemmel",
      "Manuel Kranzl",
      "Adam Lamine",
      "Philipp Neubauer",
      "Radu Grosu",
      "Sophie Neubauer"
    ],
    "author_count": 6,
    "categories": [
      "cs.CE",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CE",
    "keywords": [
      "online",
      "ssms",
      "fine-tuning",
      "carbon",
      "emission",
      "predictions",
      "real-time",
      "recurrent",
      "state",
      "space"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00804v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00804v1"
  },
  {
    "id": "2508.00785v1",
    "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors",
    "abstract": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.",
    "clean_title": "explainable ai and machine learning for exam-based student evaluation: causal and predictive analysis of socio-academic and economic factors",
    "clean_abstract": "academic performance depends on a multivariable nexus of socio-academic and financial factors. this study investigates these influences to develop effective strategies for optimizing students cgpa. to achieve this, we reviewed various literature to identify key influencing factors and constructed an initial hypothetical causal graph based on the findings. additionally, an online survey was conducted, where 1,050 students participated, providing comprehensive data for analysis. rigorous data preprocessing techniques, including cleaning and visualization, ensured data quality before analysis. causal analysis validated the relationships among variables, offering deeper insights into their direct and indirect effects on cgpa. regression models were implemented for cgpa prediction, while classification models categorized students based on performance levels. ridge regression demonstrated strong predictive accuracy, achieving a mean absolute error of 0.12 and a mean squared error of 0.023. random forest outperformed in classification, attaining an f1-score near perfection and an accuracy of 98.68. explainable ai techniques such as shap, lime, and interpret enhanced model interpretability, highlighting critical factors such as study hours, scholarships, parental education, and prior academic performance. the study culminated in the development of a web-based application that provides students with personalized insights, allowing them to predict academic performance, identify areas for improvement, and make informed decisions to enhance their outcomes.",
    "authors": [
      "Bushra Akter",
      "Md Biplob Hosen",
      "Sabbir Ahmed",
      "Mehrin Anannya",
      "Md. Farhad Hossain"
    ],
    "author_count": 5,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "students",
      "causal",
      "factors",
      "academic",
      "study",
      "data",
      "explainable",
      "predictive",
      "analysis",
      "socio-academic"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00785v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00785v1"
  },
  {
    "id": "2508.00775v1",
    "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms",
    "abstract": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.",
    "clean_title": "learning to optimize with guarantees: a complete characterization of linearly convergent algorithms",
    "clean_abstract": "in high-stakes engineering applications, optimization algorithms must come with provable worst-case guarantees over a mathematically defined class of problems. designing for the worst case, however, inevitably sacrifices performance on the specific problem instances that often occur in practice. we address the problem of augmenting a given linearly convergent algorithm to improve its average-case performance on a restricted set of target problems - for example, tailoring an off-the-shelf solver for model predictive control mpc for an application to a specific dynamical system - while preserving its worst-case guarantees across the entire problem class. toward this goal, we characterize the class of algorithms that achieve linear convergence for classes of nonsmooth composite optimization problems. in particular, starting from a baseline linearly convergent algorithm, we derive all - and only - the modifications to its update rule that maintain its convergence properties. our results apply to augmenting legacy algorithms such as gradient descent for nonconvex, gradient-dominated functions; nesterovs accelerated method for strongly convex functions; and projected methods for optimization over polyhedral feasibility sets. we showcase effectiveness of the approach on solving optimization problems with tight iteration budgets in application to ill-conditioned systems of linear equations and mpc for linear systems.",
    "authors": [
      "Andrea Martin",
      "Ian R. Manchester",
      "Luca Furieri"
    ],
    "author_count": 3,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "keywords": [
      "algorithms",
      "optimization",
      "its",
      "linearly",
      "convergent",
      "problem",
      "linear",
      "worst-case",
      "guarantees",
      "over"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00775v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00775v1"
  },
  {
    "id": "2508.00768v1",
    "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy",
    "abstract": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.",
    "clean_title": "evaluating angle and amplitude encoding strategies for variational quantum machine learning: their impact on models accuracy",
    "clean_abstract": "recent advancements in quantum computing and machine learning have increased attention to quantum machine learning qml, which aims to develop machine learning models by exploiting the quantum computing paradigm. one of the widely used models in this area is the variational quantum circuit vqc, a hybrid model where the quantum circuit handles data inference while classical optimization adjusts the parameters of the circuit. the quantum circuit consists of an encoding layer, which loads data into the circuit, and a template circuit, known as the ansatz, responsible for processing the data. this work involves performing an analysis by considering both amplitude- and angle-encoding models, and examining how the type of rotational gate applied affects the classification performance of the model. this comparison is carried out by training the different models on two datasets, wine and diabetes, and evaluating their performance. the study demonstrates that, under identical model topologies, the difference in accuracy between the best and worst models ranges from 10 to 30, with differences reaching up to 41. moreover, the results highlight how the choice of rotational gates used in encoding can significantly impact the models classification performance. the findings confirm that the embedding represents a hyperparameter for vqc models.",
    "authors": [
      "Antonio Tudisco",
      "Andrea Marchesin",
      "Maurizio Zamboni",
      "Mariagrazia Graziano",
      "Giovanna Turvani"
    ],
    "author_count": 5,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "quantum",
      "models",
      "machine",
      "encoding",
      "learning",
      "circuit",
      "evaluating",
      "variational",
      "their",
      "impact"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00768v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00768v1"
  },
  {
    "id": "2508.00823v1",
    "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
    "abstract": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
    "clean_title": "igl-nav: incremental 3d gaussian localization for image-goal navigation",
    "clean_abstract": "visual navigation with an image as goal is a fundamental and challenging problem. conventional methods either rely on end-to-end rl learning or modular-based policy with topological graph or bev map as memory, which cannot fully model the geometric relationship between the explored 3d environment and the goal image. in order to efficiently and accurately localize the goal image in 3d space, we build our navigation system upon the renderable 3d gaussian 3dgs representation. however, due to the computational intensity of 3dgs optimization and the large search space of 6-dof camera pose, directly leveraging 3dgs for image localization during agent exploration process is prohibitively inefficient. to this end, we propose igl-nav, an incremental 3d gaussian localization framework for efficient and 3d-aware image-goal navigation. specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3d convolution. when the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. the proposed igl-nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. it can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. project page: https:gwxuan.github.ioigl-nav.",
    "authors": [
      "Wenxuan Guo",
      "Xiuwei Xu",
      "Hang Yin",
      "Ziwei Wang",
      "Jianjiang Feng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "author_count": 7,
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "goal",
      "image",
      "gaussian",
      "localization",
      "image-goal",
      "navigation",
      "3dgs",
      "incremental",
      "challenging",
      "methods"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00823v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00823v1"
  },
  {
    "id": "2508.00822v1",
    "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning",
    "abstract": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.",
    "clean_title": "cross-dataset semantic segmentation performance analysis: unifying nist point cloud city datasets for 3d deep learning",
    "clean_abstract": "this study analyzes semantic segmentation performance across heterogeneously labeled point-cloud datasets relevant to public safety applications, including pre-incident planning systems derived from lidar scans. using nists point cloud city dataset enfield and memphis collections, we investigate challenges in unifying differently labeled 3d data. our methodology employs a graded schema with the kpconv architecture, evaluating performance through iou metrics on safety-relevant features. results indicate performance variability: geometrically large objects e.g. stairs, windows achieve higher segmentation performance, suggesting potential for navigational context, while smaller safety-critical features exhibit lower recognition rates. performance is impacted by class imbalance and the limited geometric distinction of smaller objects in typical lidar scans, indicating limitations in detecting certain safety-relevant features using current point-cloud methods. key identified challenges include insufficient labeled data, difficulties in unifying class labels across datasets, and the need for standardization. potential directions include automated labeling and multi-dataset learning strategies. we conclude that reliable point-cloud semantic segmentation for public safety necessitates standardized annotation protocols and improved labeling techniques to address data heterogeneity and the detection of small, safety-critical elements.",
    "authors": [
      "Alexander Nikitas Dimopoulos",
      "Joseph Grasso"
    ],
    "author_count": 2,
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "performance",
      "segmentation",
      "semantic",
      "unifying",
      "labeled",
      "point-cloud",
      "point",
      "cloud",
      "city",
      "datasets"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00822v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00822v1"
  },
  {
    "id": "2508.00782v1",
    "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation",
    "abstract": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.",
    "clean_title": "spa2v: harnessing spatial auditory cues for audio-driven spatially-aware video generation",
    "clean_abstract": "audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. however, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. in contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. this useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. as prior methods largely ignore this factor, we present spa2v, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. spa2v decomposes the generation process into two stages: 1 audio-guided video planning: we meticulously adapt a state-of-the-art mllm for a novel task of harnessing spatial and semantic cues from input audio to construct video scene layouts vsls. this serves as an intermediate representation to bridge the gap between the audio and video modalities. 2 layout-grounded video generation: we develop an efficient and effective approach to seamlessly integrate vsls as conditional guidance into pre-trained diffusion models, enabling vsl-grounded video generation in a training-free manner. extensive experiments demonstrate that spa2v excels in generating realistic videos with semantic and spatial alignment to the input audios.",
    "authors": [
      "Kien T. Pham",
      "Yingqing He",
      "Yazhou Xing",
      "Qifeng Chen",
      "Long Chen"
    ],
    "author_count": 5,
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.GR",
    "keywords": [
      "spatial",
      "video",
      "semantic",
      "generation",
      "videos",
      "from",
      "auditory",
      "cues",
      "input",
      "audio"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00782v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00782v1"
  },
  {
    "id": "2508.00777v1",
    "title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning",
    "abstract": "Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.",
    "clean_title": "zero-shot anomaly detection with dual-branch prompt learning",
    "clean_abstract": "zero-shot anomaly detection zsad enables identifying and localizing defects in unseen categories by relying solely on generalizable features rather than requiring any labeled examples of anomalies. however, existing zsad methods, whether using fixed or learned prompts, struggle under domain shifts because their training data are derived from limited training domains and fail to generalize to new distributions. in this paper, we introduce pilot, a framework designed to overcome these challenges through two key innovations: 1 a novel dual-branch prompt learning mechanism that dynamically integrates a pool of learnable prompts with structured semantic attributes, enabling the model to adaptively weight the most relevant anomaly cues for each input image; and 2 a label-free test-time adaptation strategy that updates the learnable prompt parameters using high-confidence pseudo-labels from unlabeled test data. extensive experiments on 13 industrial and medical benchmarks demonstrate that pilot achieves state-of-the-art performance in both anomaly detection and localization under domain shift.",
    "authors": [
      "Zihan Wang",
      "Samira Ebrahimi Kahou",
      "Narges Armanfard"
    ],
    "author_count": 3,
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "anomaly",
      "detection",
      "prompt",
      "zero-shot",
      "dual-branch",
      "learning",
      "zsad",
      "under",
      "domain",
      "training"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00777v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00777v1"
  },
  {
    "id": "2508.00766v1",
    "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation",
    "abstract": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.",
    "clean_title": "sample-aware test-time adaptation for medical image-to-image translation",
    "clean_abstract": "image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. however, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. to address this limitation, we propose a novel test-time adaptation tta framework that dynamically adjusts the translation process based on the characteristics of each test sample. our method introduces a reconstruction module to quantify the domain shift and a dynamic adaptation block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. we evaluate our approach on two medical image-to-image translation tasks: low-dose ct denoising and t1 to t2 mri translation, showing consistent improvements over both the baseline translation model without tta and prior tta methods. our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. the code is available at: https:github.comcosbidevsample-aware_tta.",
    "authors": [
      "Irene Iele",
      "Francesco Di Feola",
      "Valerio Guarrasi",
      "Paolo Soda"
    ],
    "author_count": 4,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "translation",
      "adaptation",
      "medical",
      "image-to-image",
      "without",
      "tta",
      "model",
      "test-time",
      "denoising",
      "limitations"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00766v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00766v1"
  },
  {
    "id": "2508.00755v1",
    "title": "AI-Driven Collaborative Satellite Object Detection for Space Sustainability",
    "abstract": "The growing density of satellites in low-Earth orbit (LEO) presents serious\nchallenges to space sustainability, primarily due to the increased risk of\nin-orbit collisions. Traditional ground-based tracking systems are constrained\nby latency and coverage limitations, underscoring the need for onboard,\nvision-based space object detection (SOD) capabilities. In this paper, we\npropose a novel satellite clustering framework that enables the collaborative\nexecution of deep learning (DL)-based SOD tasks across multiple satellites. To\nsupport this approach, we construct a high-fidelity dataset simulating imaging\nscenarios for clustered satellite formations. A distance-aware viewpoint\nselection strategy is introduced to optimize detection performance, and recent\nDL models are used for evaluation. Experimental results show that the\nclustering-based method achieves competitive detection accuracy compared to\nsingle-satellite and existing approaches, while maintaining a low size, weight,\nand power (SWaP) footprint. These findings underscore the potential of\ndistributed, AI-enabled in-orbit systems to enhance space situational awareness\nand contribute to long-term space sustainability.",
    "clean_title": "ai-driven collaborative satellite object detection for space sustainability",
    "clean_abstract": "the growing density of satellites in low-earth orbit leo presents serious challenges to space sustainability, primarily due to the increased risk of in-orbit collisions. traditional ground-based tracking systems are constrained by latency and coverage limitations, underscoring the need for onboard, vision-based space object detection sod capabilities. in this paper, we propose a novel satellite clustering framework that enables the collaborative execution of deep learning dl-based sod tasks across multiple satellites. to support this approach, we construct a high-fidelity dataset simulating imaging scenarios for clustered satellite formations. a distance-aware viewpoint selection strategy is introduced to optimize detection performance, and recent dl models are used for evaluation. experimental results show that the clustering-based method achieves competitive detection accuracy compared to single-satellite and existing approaches, while maintaining a low size, weight, and power swap footprint. these findings underscore the potential of distributed, ai-enabled in-orbit systems to enhance space situational awareness and contribute to long-term space sustainability.",
    "authors": [
      "Peng Hu",
      "Wenxuan Zhang"
    ],
    "author_count": 2,
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "keywords": [
      "space",
      "detection",
      "satellite",
      "collaborative",
      "object",
      "in-orbit",
      "systems",
      "sod",
      "ai-driven",
      "sustainability"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00755v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00755v1"
  },
  {
    "id": "2508.00819v1",
    "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models",
    "abstract": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
    "clean_title": "beyond fixed: variable-length denoising for diffusion large language models",
    "clean_abstract": "diffusion large language models dllms are emerging as a powerful alternative to the dominant autoregressive large language models, offering efficient parallel generation and capable global context modeling. however, the practical application of dllms is hindered by a critical architectural constraint: the need for a statically predefined generation length. this static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. while the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. to bridge this gap, we leverage these latent signals and introduce daedal, a novel training-free denoising strategy that enables dynamic adaptive length expansion for diffusion large language models. daedal operates in two phases: 1 before the denoising process, daedal starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2 during the denoising process, daedal dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. extensive experiments on dllms demonstrate that daedal achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. by resolving the static length constraint, daedal unlocks new potential for dllms, bridging a critical gap with their autoregressive counterparts and paving the way for more efficient and capable generation.",
    "authors": [
      "Jinsong Li",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Jiaqi Wang",
      "Dahua Lin"
    ],
    "author_count": 6,
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "length",
      "daedal",
      "denoising",
      "large",
      "language",
      "diffusion",
      "dllms",
      "generation",
      "performance",
      "while"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00819v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00819v1"
  },
  {
    "id": "2508.00788v1",
    "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models",
    "abstract": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
    "clean_title": "do they understand them? an updated evaluation on nonbinary pronoun handling in large language models",
    "clean_abstract": "large language models llms are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible ai. prior work, such as the misgendered benchmark, revealed significant limitations in earlier llms handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. in this study, we introduce misgendered, an extended and updated benchmark for evaluating llms pronoun fidelity. we benchmark five representative llms, gpt-4o, claude 4, deepseek-v3, qwen turbo, and qwen2.5, across zero-shot, few-shot, and gender identity inference. our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. however, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. we discuss implications, model-specific observations, and avenues for future inclusive ai research.",
    "authors": [
      "Xushuo Tang",
      "Yi Ding",
      "Zhengyi Yang",
      "Yin Chen",
      "Yongrui Gu",
      "Wenke Yang",
      "Mingchen Ju",
      "Xin Cao",
      "Yongfei Liu",
      "Wenjie Zhang"
    ],
    "author_count": 10,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "pronoun",
      "models",
      "llms",
      "updated",
      "handling",
      "large",
      "language",
      "especially",
      "gender-neutral",
      "remains"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00788v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00788v1"
  },
  {
    "id": "2508.00762v1",
    "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation",
    "abstract": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.",
    "clean_title": "itunlp at semeval-2025 task 8: question-answering over tabular data: a zero-shot approach using llm-driven code generation",
    "clean_abstract": "this paper presents our system for semeval-2025 task 8: databench, question-answering over tabular data. the primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: databench qa subtask i and databench lite qa subtask ii. to tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging large language model llm-based code generation. specifically, we propose a python code generation framework utilizing state-of-the-art open-source llms to generate executable pandas code via optimized prompting strategies. our experiments reveal that different llms exhibit varying levels of effectiveness in python code generation. additionally, results show that python code generation achieves superior performance in tabular question answering compared to alternative approaches. although our ranking among zero-shot systems is unknown at the time of this papers submission, our system achieved eighth place in subtask i and sixth place in subtaskii among the 30 systems that outperformed the baseline in the open-source models category.",
    "authors": [
      "Atakan Site",
      "Emre Hakan Erdemir",
      "Glen Eryiit"
    ],
    "author_count": 3,
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "code",
      "tabular",
      "task",
      "zero-shot",
      "generation",
      "subtask",
      "python",
      "semeval-2025",
      "question-answering",
      "over"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00762v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00762v1"
  },
  {
    "id": "2508.00760v1",
    "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations",
    "abstract": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.",
    "clean_title": "mmbert: scaled mixture-of-experts multimodal bert for robust chinese hate speech detection under cloaking perturbations",
    "clean_abstract": "hate speech detection on chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. although large language models llms have recently improved hate speech detection capabilities, the majority of existing work has concentrated on english datasets, with limited attention given to multimodal strategies in the chinese context. in this study, we propose mmbert, a novel bert-based multimodal framework that integrates textual, speech, and visual modalities through a mixture-of-experts moe architecture. to address the instability associated with directly integrating moe into bert-based models, we develop a progressive three-stage training paradigm. mmbert incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. empirical results in several chinese hate speech datasets show that mmbert significantly surpasses fine-tuned bert-based encoder models, fine-tuned llms, and llms utilizing in-context learning approaches.",
    "authors": [
      "Qiyao Xue",
      "Yuchen Dou",
      "Ryan Shi",
      "Xiang Lorraine Li",
      "Wei Gao"
    ],
    "author_count": 5,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "chinese",
      "hate",
      "speech",
      "detection",
      "multimodal",
      "bert-based",
      "mixture-of-experts",
      "cloaking",
      "llms",
      "moe"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00760v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00760v1"
  },
  {
    "id": "2508.00757v1",
    "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction",
    "abstract": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.",
    "clean_title": "glidre: generalist lightweight model for document-level relation extraction",
    "clean_abstract": "relation extraction re is a fundamental task in natural language processing, and its document-level variant poses significant challenges, due to the need to model complex interactions between entities across sentences. current approaches, largely based on the atlop architecture, are commonly evaluated on benchmarks like docred and re-docred. however, their performance in zero-shot or few-shot settings remains largely underexplored due to the tasks complexity. recently, the gliner model has shown that a compact ner model can outperform much larger large language models. with a similar motivation, we introduce glidre, a new model for document-level relation extraction that builds on the key ideas of gliner. we benchmark glidre against state-of-the-art models across various data settings on the re-docred dataset. our results demonstrate that glidre achieves state-of-the-art performance in few-shot scenarios. our code is publicly available.",
    "authors": [
      "Robin Armingaud",
      "Romaric Besanon"
    ],
    "author_count": 2,
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "model",
      "document-level",
      "relation",
      "extraction",
      "language",
      "due",
      "across",
      "largely",
      "performance",
      "few-shot"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00757v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00757v1"
  },
  {
    "id": "2508.00743v1",
    "title": "Agentic large language models improve retrieval-based radiology question answering",
    "abstract": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
    "clean_title": "agentic large language models improve retrieval-based radiology question answering",
    "clean_abstract": "clinical decision-making in radiology increasingly benefits from artificial intelligence ai, particularly through large language models llms. however, traditional retrieval-augmented generation rag systems for radiology question answering qa typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. here we propose an agentic rag framework enabling llms to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from radiopaedia, and dynamically synthesize evidence-based responses. we evaluated 24 llms spanning diverse architectures, parameter scales 0.5b to 670b, and training paradigms general-purpose, reasoning-optimized, clinically fine-tuned, using 104 expert-curated radiology questions from previously established rsna-radioqa and extendedqa datasets. agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting 73 vs. 64; p0.001 and conventional online rag 73 vs. 68; p0.001. the greatest gains occurred in mid-sized models e.g., mistral large improved from 72 to 81 and small-scale models e.g., qwen 2.5-7b improved from 55 to 71, while very large models 200b parameters demonstrated minimal changes 2 improvement. additionally, agentic retrieval reduced hallucinations mean 9.4 and retrieved clinically relevant context in 46 of cases, substantially aiding factual grounding. even clinically fine-tuned models exhibited meaningful improvements e.g., medgemma-27b improved from 71 to 81, indicating complementary roles of retrieval and fine-tuning. these results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology qa, particularly among mid-sized llms, warranting future studies to validate their clinical utility.",
    "authors": [
      "Sebastian Wind",
      "Jeta Sopa",
      "Daniel Truhn",
      "Mahshad Lotfinia",
      "Tri-Thien Nguyen",
      "Keno Bressem",
      "Lisa Adams",
      "Mirabela Rusu",
      "Harald Kstler",
      "Gerhard Wellein",
      "Andreas Maier",
      "Soroosh Tayebi Arasteh"
    ],
    "author_count": 12,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "models",
      "radiology",
      "from",
      "agentic",
      "large",
      "clinical",
      "improved",
      "rag",
      "clinically",
      "retrieval"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00743v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00743v1"
  },
  {
    "id": "2508.00756v1",
    "title": "LeakyCLIP: Extracting Training Data from CLIP",
    "abstract": "Understanding the memorization and privacy leakage risks in Contrastive\nLanguage--Image Pretraining (CLIP) is critical for ensuring the security of\nmultimodal models. Recent studies have demonstrated the feasibility of\nextracting sensitive training examples from diffusion models, with conditional\ndiffusion models exhibiting a stronger tendency to memorize and leak\ninformation. In this work, we investigate data memorization and extraction\nrisks in CLIP through the lens of CLIP inversion, a process that aims to\nreconstruct training images from text prompts. To this end, we introduce\n\\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,\nsemantically accurate image reconstruction from CLIP embeddings. We identify\nthree key challenges in CLIP inversion: 1) non-robust features, 2) limited\nvisual semantics in text embeddings, and 3) low reconstruction fidelity. To\naddress these challenges, LeakyCLIP employs 1) adversarial fine-tuning to\nenhance optimization smoothness, 2) linear transformation-based embedding\nalignment, and 3) Stable Diffusion-based refinement to improve fidelity.\nEmpirical results demonstrate the superiority of LeakyCLIP, achieving over 358%\nimprovement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared\nto baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive\nleakage risk, showing that training data membership can even be successfully\ninferred from the metrics of low-fidelity reconstructions. Our work introduces\na practical method for CLIP inversion while offering novel insights into the\nnature and scope of privacy risks in multimodal models.",
    "clean_title": "leakyclip: extracting training data from clip",
    "clean_abstract": "understanding the memorization and privacy leakage risks in contrastive language--image pretraining clip is critical for ensuring the security of multimodal models. recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. in this work, we investigate data memorization and extraction risks in clip through the lens of clip inversion, a process that aims to reconstruct training images from text prompts. to this end, we introduce textbfleakyclip, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from clip embeddings. we identify three key challenges in clip inversion: 1 non-robust features, 2 limited visual semantics in text embeddings, and 3 low reconstruction fidelity. to address these challenges, leakyclip employs 1 adversarial fine-tuning to enhance optimization smoothness, 2 linear transformation-based embedding alignment, and 3 stable diffusion-based refinement to improve fidelity. empirical results demonstrate the superiority of leakyclip, achieving over 358 improvement in structural similarity index measure ssim for vit-b-16 compared to baseline methods on laion-2b subset. furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. our work introduces a practical method for clip inversion while offering novel insights into the nature and scope of privacy risks in multimodal models.",
    "authors": [
      "Yunhao Chen",
      "Shujie Wang",
      "Xin Wang",
      "Xingjun Ma"
    ],
    "author_count": 4,
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR",
    "keywords": [
      "clip",
      "from",
      "training",
      "data",
      "risks",
      "extracting",
      "memorization",
      "privacy",
      "leakage",
      "multimodal"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00756v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00756v1"
  },
  {
    "id": "2508.00748v1",
    "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos",
    "abstract": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.",
    "clean_title": "is it really you? exploring biometric verification scenarios in photorealistic talking-head avatar videos",
    "clean_abstract": "photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. these avatars allow for more immersive communication, but they also introduce serious security risks. one emerging threat is impersonation: an attacker can steal a users avatar-preserving their appearance and voice-making it nearly impossible to detect its fraudulent usage by sight or sound alone. in this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. our main question is whether an individuals facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatars visual appearance is a facsimile of its owner. to answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, gagavatar, with genuine and impostor avatar videos. we also propose a lightweight, explainable spatio-temporal graph convolutional network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. experimental results demonstrate that facial motion cues enable meaningful identity verification with auc values approaching 80. the proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.",
    "authors": [
      "Laura Pedrouzo-Rodriguez",
      "Pedro Delgado-DeRobles",
      "Luis F. Gomez",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "author_count": 7,
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "biometric",
      "avatar",
      "facial",
      "verification",
      "avatars",
      "photorealistic",
      "talking-head",
      "videos",
      "more",
      "also"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00748v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00748v1"
  },
  {
    "id": "2508.00682v1",
    "title": "Unveiling Dynamic Binary Instrumentation Techniques",
    "abstract": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances.",
    "clean_title": "unveiling dynamic binary instrumentation techniques",
    "clean_abstract": "dynamic binary instrumentation dbi is the set of techniques that enable instrumentation of programs at run-time, making it possible to monitor and modify the execution of compiled binaries or entire systems. dbi is used for countless security applications and analyses, and is extensively used across many fields in both industry and academia. over the years, several dbi approaches have been proposed based on different technologies and implementing diverse techniques. every solution tries to overcome certain limitations, but they sometimes bring other shortcomings. some are specialized for one particular domain or task, while others have a wider scope. in this paper, we shed light into the labyrinth of dbi, bringing together process-level and whole-system approaches. we depict their building blocks and analyze the underlying instrumentation techniques, comparing their ability to instrument different primitives and run-time events. then, we evaluate their performance when implementing each primitive, and highlight relevant observations. our results show that no single technique is better than the rest in all circumstances.",
    "authors": [
      "Oscar Llorente-Vazquez",
      "Xabier Ugarte-Pedrero",
      "Igor Santos-Grueiro",
      "Pablo Garcia Bringas"
    ],
    "author_count": 4,
    "categories": [
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "keywords": [
      "instrumentation",
      "dbi",
      "their",
      "dynamic",
      "binary",
      "techniques",
      "different",
      "implementing",
      "unveiling",
      "set"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00682v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00682v1"
  },
  {
    "id": "2508.00659v1",
    "title": "Demo: TOSense -- What Did You Just Agree to?",
    "abstract": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.",
    "clean_title": "demo: tosense -- what did you just agree to?",
    "clean_abstract": "online services often require users to agree to lengthy and obscure terms of service tos, leading to information asymmetry and legal risks. this paper proposes tosense-a chrome extension that allows users to ask questions about tos in natural language and get concise answers in real time. the system combines i a crawler tos-crawl that automatically extracts tos content, and ii a lightweight large language model pipeline: minilm for semantic retrieval and bart-encoder for answer relevance verification. to avoid expensive manual annotation, we present a novel question answering evaluation pipeline qep that generates synthetic questions and verifies the correctness of answers using clustered topic matching. experiments on five major platforms, apple, google, x formerly twitter, microsoft, and netflix, show the effectiveness of tosense with up to 44.5 accuracy across varying number of topic clusters. during the demonstration, we will showcase tosense in action. attendees will be able to experience seamless extraction, interactive question answering, and instant indexing of new sites.",
    "authors": [
      "Xinzhang Chen",
      "Hassan Ali",
      "Arash Shaghaghi",
      "Salil S. Kanhere",
      "Sanjay Jha"
    ],
    "author_count": 5,
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "keywords": [
      "tosense",
      "agree",
      "users",
      "questions",
      "tos",
      "language",
      "answers",
      "question",
      "topic",
      "demo:"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00659v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00659v1"
  },
  {
    "id": "2508.00637v1",
    "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks",
    "abstract": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios.",
    "clean_title": "cyber-physical co-simulation of load frequency control under load-altering attacks",
    "clean_abstract": "integrating information and communications technology ict devices into the power grid brings many benefits. however, it also exposes the grid to new potential cyber threats. many control and protection mechanisms, such as load frequency control lfc, responsible for maintaining nominal frequency during load fluctuations and under frequency load shedding ufls disconnecting portion of the load during an emergency, are dependent on information exchange through the communication network. the recently emerging load altering attacks laas utilize a botnet of high-wattage devices to introduce load fluctuation. in their dynamic form dlaas, they manipulate the load in response to live grid frequency measurements for increased efficiency, posing a notable threat to grid stability. recognizing the importance of communication networks in power grid cyber security research, this paper presents an open-source co-simulation environment that models the power grid with the corresponding communication network, implementing grid protective mechanisms. this setup allows the comprehensive analysis of the attacks in concrete lfc and ufls scenarios.",
    "authors": [
      "Micha Forystek",
      "Andrew D. Syrmakesis",
      "Alkistis Kontou",
      "Panos Kotsampopoulos",
      "Nikos D. Hatziargyriou",
      "Charalambos Konstantinou"
    ],
    "author_count": 6,
    "categories": [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "keywords": [
      "load",
      "grid",
      "frequency",
      "control",
      "attacks",
      "power",
      "communication",
      "co-simulation",
      "under",
      "information"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00637v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00637v1"
  },
  {
    "id": "2508.00636v1",
    "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients",
    "abstract": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks.",
    "clean_title": "fedguard: a diverse-byzantine-robust mechanism for federated learning with major malicious clients",
    "clean_abstract": "federated learning is a distributed training framework vulnerable to byzantine attacks, particularly when over 50 of clients are malicious or when datasets are highly non-independent and identically distributed non-iid. additionally, most existing defense mechanisms are designed for specific attack types e.g., gradient similarity-based schemes can only defend against outlier model poisoning, limiting their effectiveness. in response, we propose fedguard, a novel federated learning mechanism. fedguard cleverly addresses the aforementioned issues by leveraging the high sensitivity of membership inference to model bias. by requiring clients to include an additional mini-batch of server-specified data in their training, fedguard can identify and exclude poisoned models, as their confidence in the mini-batch will drop significantly. our comprehensive evaluation unequivocally shows that, under three highly non-iid datasets, with 90 of clients being byzantine and seven different types of byzantine attacks occurring in each round, fedguard significantly outperforms existing robust federated learning schemes in mitigating various types of byzantine attacks.",
    "authors": [
      "Haocheng Jiang",
      "Hua Shen",
      "Jixin Zhang",
      "Willy Susilo",
      "Mingwu Zhang"
    ],
    "author_count": 5,
    "categories": [
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "keywords": [
      "federated",
      "learning",
      "clients",
      "byzantine",
      "types",
      "their",
      "fedguard",
      "malicious",
      "distributed",
      "when"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00636v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00636v1"
  },
  {
    "id": "2508.00217v1",
    "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges",
    "abstract": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.",
    "clean_title": "tabular data understanding with llms: a survey of recent advances and challenges",
    "clean_abstract": "tables have gained significant attention in large language models llms and multimodal large language models mllms due to their complex and flexible structure. unlike linear text inputs, tables are two-dimensional, encompassing formats that range from well-structured database tables to complex, multi-layered spreadsheets, each with different purposes. this diversity in format and purpose has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging. to address these challenges, this paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. we highlight several critical gaps in the field that indicate the need for further research: 1 the predominance of retrieval-focused tasks that require minimal reasoning beyond mathematical and logical operations; 2 significant challenges faced by models when processing complex table structures, large-scale tables, length context, or multi-table scenarios; and 3 the limited generalization of models across different tabular representations and formats.",
    "authors": [
      "Xiaofeng Wu",
      "Alan Ritter",
      "Wei Xu"
    ],
    "author_count": 3,
    "categories": [
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "models",
      "tabular",
      "understanding",
      "tables",
      "table",
      "challenges",
      "significant",
      "large",
      "language",
      "complex"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2508.00217v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00217v1"
  },
  {
    "id": "2507.23515v1",
    "title": "DataLens: Enhancing Dataset Discovery via Network Topologies",
    "abstract": "The rapid growth of publicly available textual resources, such as lexicons\nand domain-specific corpora, presents challenges in efficiently identifying\nrelevant resources. While repositories are emerging, they often lack advanced\nsearch and exploration features. Most search methods rely on keyword queries\nand metadata filtering, which require prior knowledge and fail to reveal\nconnections between resources. To address this, we present DataLens, a\nweb-based platform that combines faceted search with advanced visualization\ntechniques to enhance resource discovery. DataLens offers network-based\nvisualizations, where the network structure can be adapted to suit the specific\nanalysis task. It also supports a chained views approach, enabling users to\nexplore data from multiple perspectives. A formative user study involving six\ndata practitioners revealed that users highly value visualization\ntools-especially network-based exploration-and offered insights to help refine\nour approach to better support dataset search.",
    "clean_title": "datalens: enhancing dataset discovery via network topologies",
    "clean_abstract": "the rapid growth of publicly available textual resources, such as lexicons and domain-specific corpora, presents challenges in efficiently identifying relevant resources. while repositories are emerging, they often lack advanced search and exploration features. most search methods rely on keyword queries and metadata filtering, which require prior knowledge and fail to reveal connections between resources. to address this, we present datalens, a web-based platform that combines faceted search with advanced visualization techniques to enhance resource discovery. datalens offers network-based visualizations, where the network structure can be adapted to suit the specific analysis task. it also supports a chained views approach, enabling users to explore data from multiple perspectives. a formative user study involving six data practitioners revealed that users highly value visualization tools-especially network-based exploration-and offered insights to help refine our approach to better support dataset search.",
    "authors": [
      "Anas Ollagnier",
      "Aline Menin"
    ],
    "author_count": 2,
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB",
    "keywords": [
      "search",
      "dataset",
      "network",
      "resources.",
      "advanced",
      "visualization",
      "network-based",
      "users",
      "data",
      "datalens:"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23515v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23515v1"
  },
  {
    "id": "2507.23499v1",
    "title": "Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets",
    "abstract": "Recording data changes in RDF systems is a crucial capability, needed to\nsupport auditing, incremental backups, database replication, and event-driven\nworkflows. In large-scale and low-latency RDF applications, the high volume and\nfrequency of updates can cause performance bottlenecks in the serialization and\ntransmission of changes. To alleviate this, we propose Jelly-Patch -- a\nhigh-performance, compressed binary serialization format for changes in RDF\ndatasets. To evaluate its performance, we benchmark Jelly-Patch against\nexisting RDF Patch formats, using two datasets representing different use cases\n(change data capture and IoT streams). Jelly-Patch is shown to achieve\n3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in\nserialization and parsing, respectively. These significant advancements in\nthroughput and compression are expected to improve the performance of\nlarge-scale and low-latency RDF systems.",
    "clean_title": "jelly-patch: a fast format for recording changes in rdf datasets",
    "clean_abstract": "recording data changes in rdf systems is a crucial capability, needed to support auditing, incremental backups, database replication, and event-driven workflows. in large-scale and low-latency rdf applications, the high volume and frequency of updates can cause performance bottlenecks in the serialization and transmission of changes. to alleviate this, we propose jelly-patch -- a high-performance, compressed binary serialization format for changes in rdf datasets. to evaluate its performance, we benchmark jelly-patch against existing rdf patch formats, using two datasets representing different use cases change data capture and iot streams. jelly-patch is shown to achieve 3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in serialization and parsing, respectively. these significant advancements in throughput and compression are expected to improve the performance of large-scale and low-latency rdf systems.",
    "authors": [
      "Piotr Sowinski",
      "Kacper Grzymkowski",
      "Anastasiya Danilenka"
    ],
    "author_count": 3,
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB",
    "keywords": [
      "rdf",
      "changes",
      "serialization",
      "jelly-patch",
      "format",
      "recording",
      "datasets",
      "data",
      "large-scale",
      "low-latency"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23499v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23499v1"
  },
  {
    "id": "2507.23429v1",
    "title": "Chatting with your ERP: A Recipe",
    "abstract": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.",
    "clean_title": "chatting with your erp: a recipe",
    "clean_abstract": "this paper presents the design, implementation, and evaluation behind a large language model llm agent that chats with an industrial production-grade erp system. the agent is capable of interpreting natural language queries and translating them into executable sql statements, leveraging open-weight llms. a novel dual-agent architecture combining reasoning and critique stages was proposed to improve query generation reliability.",
    "authors": [
      "Jorge Ruiz Gmez",
      "Lidia Andrs Susinos",
      "Jorge Alamo Oliv",
      "Sonia Rey Osorno",
      "Manuel Luis Gonzalez Hernndez"
    ],
    "author_count": 5,
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.ET",
      "cs.HC",
      "cs.MA",
      "68T50, 68P20",
      "I.2.7; H.2.5; H.2.8; H.5.m"
    ],
    "primary_category": "cs.AI",
    "keywords": [
      "language",
      "agent",
      "chatting",
      "your",
      "erp:",
      "recipe",
      "presents",
      "design,",
      "implementation,",
      "evaluation"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23429v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23429v1"
  },
  {
    "id": "2507.23358v1",
    "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction",
    "abstract": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.",
    "clean_title": "text-to-sql task-oriented dialogue ontology construction",
    "clean_abstract": "large language models llms are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. in task-oriented dialogue tod systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. however, building such ontologies requires manual labels or supervised training. we introduce teqodo: a text-to-sql task-oriented dialogue ontology construction method. here, an llm autonomously builds a tod ontology from scratch without supervision using its inherent sql programming capabilities combined with dialogue theory provided in the prompt. we show that teqodo outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. ablation studies demonstrate the key role of dialogue theory. teqodo also scales to allow construction of much larger ontologies, which we investigate on a wikipedia and arxiv dataset. we view this as a step towards broader application of ontologies to increase llm explainability.",
    "authors": [
      "Renato Vukovic",
      "Carel van Niekerk",
      "Michael Heck",
      "Benjamin Ruppik",
      "Hsien-Chin Lin",
      "Shutong Feng",
      "Nurul Lubis",
      "Milica Gasic"
    ],
    "author_count": 8,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "dialogue",
      "ontology",
      "task-oriented",
      "construction",
      "text-to-sql",
      "explainability",
      "tod",
      "ontologies",
      "llm",
      "its"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23358v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23358v1"
  },
  {
    "id": "2507.23084v1",
    "title": "AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads",
    "abstract": "Efficiently selecting indexes is fundamental to database performance\noptimization, particularly for systems handling large-scale analytical\nworkloads. While deep reinforcement learning (DRL) has shown promise in\nautomating index selection through its ability to learn from experience, few\nworks address how these RL-based index advisors can adapt to scaling workloads\ndue to exponentially growing action spaces and heavy trial and error. To\naddress these challenges, we introduce AutoIndexer, a framework that combines\nworkload compression, query optimization, and specialized RL models to scale\nindex selection effectively. By operating on compressed workloads, AutoIndexer\nsubstantially lowers search complexity without sacrificing much index quality.\nExtensive evaluations show that it reduces end-to-end query execution time by\nup to 95% versus non-indexed baselines. On average, it outperforms\nstate-of-the-art RL-based index advisors by approximately 20% in workload cost\nsavings while cutting tuning time by over 50%. These results affirm\nAutoIndexer's practicality for large and diverse workloads.",
    "clean_title": "autoindexer: a reinforcement learning-enhanced index advisor towards scaling workloads",
    "clean_abstract": "efficiently selecting indexes is fundamental to database performance optimization, particularly for systems handling large-scale analytical workloads. while deep reinforcement learning drl has shown promise in automating index selection through its ability to learn from experience, few works address how these rl-based index advisors can adapt to scaling workloads due to exponentially growing action spaces and heavy trial and error. to address these challenges, we introduce autoindexer, a framework that combines workload compression, query optimization, and specialized rl models to scale index selection effectively. by operating on compressed workloads, autoindexer substantially lowers search complexity without sacrificing much index quality. extensive evaluations show that it reduces end-to-end query execution time by up to 95 versus non-indexed baselines. on average, it outperforms state-of-the-art rl-based index advisors by approximately 20 in workload cost savings while cutting tuning time by over 50. these results affirm autoindexers practicality for large and diverse workloads.",
    "authors": [
      "Taiyi Wang",
      "Eiko Yoneki"
    ],
    "author_count": 2,
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "keywords": [
      "index",
      "reinforcement",
      "scaling",
      "workloads",
      "optimization,",
      "workloads.",
      "while",
      "selection",
      "address",
      "rl-based"
    ],
    "published_date": "2025-07-30",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23084v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23084v1"
  },
  {
    "id": "2508.00806v1",
    "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management",
    "abstract": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.",
    "clean_title": "adacc: adaptive compression and activation checkpointing for llm memory management",
    "clean_abstract": "training large language models often employs recomputation to alleviate memory pressure, which can introduce up to 30 overhead in real-world scenarios. in this paper, we propose adacc, a novel memory management framework that combines adaptive compression and activation checkpointing to reduce the gpu memory footprint. it comprises three modules: 1 we design layer-specific compression algorithms that account for outliers in llm tensors, instead of directly quantizing floats from fp16 to int4, to ensure model accuracy. 2 we propose an optimal scheduling policy that employs milp to determine the best memory optimization for each tensor. 3 to accommodate changes in training tensors, we introduce an adaptive policy evolution mechanism that adjusts the policy during training to enhance throughput. experimental results show that adacc can accelerate the llm training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy to the baseline.",
    "authors": [
      "Ping Chen",
      "Zhuohong Deng",
      "Ping Li",
      "Shuibing He",
      "Hongzi Zhu",
      "Yi Zheng",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Minyi Guo"
    ],
    "author_count": 9,
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "memory",
      "training",
      "adaptive",
      "compression",
      "llm",
      "policy",
      "activation",
      "checkpointing",
      "management",
      "employs"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00806v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00806v1"
  },
  {
    "id": "2508.00636v1",
    "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients",
    "abstract": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks.",
    "clean_title": "fedguard: a diverse-byzantine-robust mechanism for federated learning with major malicious clients",
    "clean_abstract": "federated learning is a distributed training framework vulnerable to byzantine attacks, particularly when over 50 of clients are malicious or when datasets are highly non-independent and identically distributed non-iid. additionally, most existing defense mechanisms are designed for specific attack types e.g., gradient similarity-based schemes can only defend against outlier model poisoning, limiting their effectiveness. in response, we propose fedguard, a novel federated learning mechanism. fedguard cleverly addresses the aforementioned issues by leveraging the high sensitivity of membership inference to model bias. by requiring clients to include an additional mini-batch of server-specified data in their training, fedguard can identify and exclude poisoned models, as their confidence in the mini-batch will drop significantly. our comprehensive evaluation unequivocally shows that, under three highly non-iid datasets, with 90 of clients being byzantine and seven different types of byzantine attacks occurring in each round, fedguard significantly outperforms existing robust federated learning schemes in mitigating various types of byzantine attacks.",
    "authors": [
      "Haocheng Jiang",
      "Hua Shen",
      "Jixin Zhang",
      "Willy Susilo",
      "Mingwu Zhang"
    ],
    "author_count": 5,
    "categories": [
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "keywords": [
      "federated",
      "learning",
      "clients",
      "byzantine",
      "types",
      "their",
      "fedguard",
      "malicious",
      "distributed",
      "when"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00636v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00636v1"
  },
  {
    "id": "2508.00622v1",
    "title": "SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments",
    "abstract": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical\napplications such as aerial mapping, environmental monitoring, and autonomous\ndelivery. However, the reliability of these systems is highly dependent on\nuninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,\nwhich can be disrupted in real-world scenarios due to interference,\nenvironmental conditions, or adversarial attacks, causing disorientation,\ncollision risks, and mission failure. This paper proposes SwarnRaft, a\nblockchain-inspired positioning and consensus framework for maintaining\ncoordination and data integrity in UAV swarms operating under GNSS-denied\nconditions. SwarnRaft leverages the Raft consensus algorithm to enable\ndistributed drones (nodes) to agree on state updates such as location and\nheading, even in the absence of GNSS signals for one or more nodes. In our\nprototype, each node uses GNSS and local sensing, and communicates over WiFi in\na simulated swarm. Upon signal loss, consensus is used to reconstruct or verify\nthe position of the failed node based on its last known state and trajectory.\nOur system demonstrates robustness in maintaining swarm coherence and fault\ntolerance through a lightweight, scalable communication model. This work offers\na practical and secure foundation for decentralized drone operation in\nunpredictable environments.",
    "clean_title": "swarnraft: leveraging consensus for robust drone swarm coordination in gnss-degraded environments",
    "clean_abstract": "unmanned aerial vehicle uav swarms are increasingly used in critical applications such as aerial mapping, environmental monitoring, and autonomous delivery. however, the reliability of these systems is highly dependent on uninterrupted access to the global navigation satellite systems gnss signals, which can be disrupted in real-world scenarios due to interference, environmental conditions, or adversarial attacks, causing disorientation, collision risks, and mission failure. this paper proposes swarnraft, a blockchain-inspired positioning and consensus framework for maintaining coordination and data integrity in uav swarms operating under gnss-denied conditions. swarnraft leverages the raft consensus algorithm to enable distributed drones nodes to agree on state updates such as location and heading, even in the absence of gnss signals for one or more nodes. in our prototype, each node uses gnss and local sensing, and communicates over wifi in a simulated swarm. upon signal loss, consensus is used to reconstruct or verify the position of the failed node based on its last known state and trajectory. our system demonstrates robustness in maintaining swarm coherence and fault tolerance through a lightweight, scalable communication model. this work offers a practical and secure foundation for decentralized drone operation in unpredictable environments.",
    "authors": [
      "Kapel Dev",
      "Yash Madhwal",
      "Sofia Shevelo",
      "Pavel Osinenko",
      "Yury Yanovich"
    ],
    "author_count": 5,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "keywords": [
      "consensus",
      "gnss",
      "drone",
      "swarm",
      "coordination",
      "aerial",
      "uav",
      "swarms",
      "such",
      "environmental"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00622v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00622v1"
  },
  {
    "id": "2508.00596v1",
    "title": "Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience",
    "abstract": "In decentralized federated learning (FL), multiple clients collaboratively\nlearn a shared machine learning (ML) model by leveraging their privately held\ndatasets distributed across the network, through interactive exchange of the\nintermediate model updates. To ensure data security, cryptographic techniques\nare commonly employed to protect model updates during aggregation. Despite\ngrowing interest in secure aggregation, existing works predominantly focus on\nprotocol design and computational guarantees, with limited understanding of the\nfundamental information-theoretic limits of such systems. Moreover, optimal\nbounds on communication and key usage remain unknown in decentralized settings,\nwhere no central aggregator is available. Motivated by these gaps, we study the\nproblem of decentralized secure aggregation (DSA) from an information-theoretic\nperspective. Specifically, we consider a network of $K$ fully-connected users,\neach holding a private input -- an abstraction of local training data -- who\naim to securely compute the sum of all inputs. The security constraint requires\nthat no user learns anything beyond the input sum, even when colluding with up\nto $T$ other users. We characterize the optimal rate region, which specifies\nthe minimum achievable communication and secret key rates for DSA. In\nparticular, we show that to securely compute one symbol of the desired input\nsum, each user must (i) transmit at least one symbol to others, (ii) hold at\nleast one symbol of secret key, and (iii) all users must collectively hold no\nfewer than $K - 1$ independent key symbols. Our results establish the\nfundamental performance limits of DSA, providing insights for the design of\nprovably secure and communication-efficient protocols in distributed learning\nsystems.",
    "clean_title": "information-theoretic decentralized secure aggregation with collusion resilience",
    "clean_abstract": "in decentralized federated learning fl, multiple clients collaboratively learn a shared machine learning ml model by leveraging their privately held datasets distributed across the network, through interactive exchange of the intermediate model updates. to ensure data security, cryptographic techniques are commonly employed to protect model updates during aggregation. despite growing interest in secure aggregation, existing works predominantly focus on protocol design and computational guarantees, with limited understanding of the fundamental information-theoretic limits of such systems. moreover, optimal bounds on communication and key usage remain unknown in decentralized settings, where no central aggregator is available. motivated by these gaps, we study the problem of decentralized secure aggregation dsa from an information-theoretic perspective. specifically, we consider a network of k fully-connected users, each holding a private input -- an abstraction of local training data -- who aim to securely compute the sum of all inputs. the security constraint requires that no user learns anything beyond the input sum, even when colluding with up to t other users. we characterize the optimal rate region, which specifies the minimum achievable communication and secret key rates for dsa. in particular, we show that to securely compute one symbol of the desired input sum, each user must i transmit at least one symbol to others, ii hold at least one symbol of secret key, and iii all users must collectively hold no fewer than k - 1 independent key symbols. our results establish the fundamental performance limits of dsa, providing insights for the design of provably secure and communication-efficient protocols in distributed learning systems.",
    "authors": [
      "Xiang Zhang",
      "Zhou Li",
      "Shuangyang Li",
      "Kai Wan",
      "Derrick Wing Kwan Ng",
      "Giuseppe Caire"
    ],
    "author_count": 6,
    "categories": [
      "cs.IT",
      "cs.CR",
      "cs.DC",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "keywords": [
      "decentralized",
      "secure",
      "information-theoretic",
      "learning",
      "model",
      "key",
      "input",
      "one",
      "symbol",
      "aggregation"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00596v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00596v1"
  },
  {
    "id": "2508.00426v1",
    "title": "Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services",
    "abstract": "Conference services like Zoom, Microsoft Teams, and Google Meet facilitate\nmillions of daily calls, yet ensuring high performance at low costs remains a\nsignificant challenge. This paper revisits the problem of packing calls across\nMedia Processor (MP) servers that host the calls within individual datacenters\n(DCs). We show that the algorithm used in Teams -- a large scale conferencing\nservice as well as other state-of-art algorithms are prone to placing calls\nresulting in some of the MPs becoming hot (high CPU utilization) that leads to\ndegraded performance and/or elevated hosting costs. The problem arises from\ndisregarding the variability in CPU usage among calls, influenced by\ndifferences in participant numbers and media types (audio/video), compounded by\nbursty call arrivals. To tackle this, we propose Tetris, a multi-step framework\nwhich (a) optimizes initial call assignments by leveraging historical data and\n(b) periodically migrates calls from hot MPs using linear optimization, aiming\nto minimize hot MP usage. Evaluation based on a 24-hour trace of over 10\nmillion calls in one DC shows that Tetris reduces participant numbers on hot\nMPs by at least 2.5X.",
    "clean_title": "tetris: efficient intra-datacenter calls packing for large conferencing services",
    "clean_abstract": "conference services like zoom, microsoft teams, and google meet facilitate millions of daily calls, yet ensuring high performance at low costs remains a significant challenge. this paper revisits the problem of packing calls across media processor mp servers that host the calls within individual datacenters dcs. we show that the algorithm used in teams -- a large scale conferencing service as well as other state-of-art algorithms are prone to placing calls resulting in some of the mps becoming hot high cpu utilization that leads to degraded performance andor elevated hosting costs. the problem arises from disregarding the variability in cpu usage among calls, influenced by differences in participant numbers and media types audiovideo, compounded by bursty call arrivals. to tackle this, we propose tetris, a multi-step framework which a optimizes initial call assignments by leveraging historical data and b periodically migrates calls from hot mps using linear optimization, aiming to minimize hot mp usage. evaluation based on a 24-hour trace of over 10 million calls in one dc shows that tetris reduces participant numbers on hot mps by at least 2.5x.",
    "authors": [
      "Rohan Gandhi",
      "Ankur Mallick",
      "Ken Sueda",
      "Rui Liang"
    ],
    "author_count": 4,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "keywords": [
      "calls",
      "hot",
      "mps",
      "packing",
      "large",
      "conferencing",
      "services",
      "calls,",
      "high",
      "performance"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00426v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00426v1"
  },
  {
    "id": "2508.00341v1",
    "title": "Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT",
    "abstract": "The rising popularity of Internet of things (IoT) has spurred technological\nadvancements in mobile internet and interconnected systems. While offering\nflexible connectivity and intelligent applications across various domains, IoT\nservice providers must gather vast amounts of sensitive data from users, which\nnonetheless concomitantly raises concerns about privacy breaches. Federated\nlearning (FL) has emerged as a promising decentralized training paradigm to\ntackle this challenge. This work focuses on enhancing the aggregation\nefficiency of distributed local models by introducing over-the-air computation\ninto the FL framework. Due to radio resource scarcity in large-scale networks,\nonly a subset of users can participate in each training round. This highlights\nthe need for effective user scheduling and model transmission strategies to\noptimize communication efficiency and inference accuracy. To address this, we\npropose an integrated approach to user scheduling and receive beam steering,\nsubject to constraints on the number of selected users and transmit power.\nLeveraging the difference-of-convex technique, we decompose the primal\nnon-convex optimization problem into two sub-problems, yielding an iterative\nsolution. While effective, the computational load of the iterative method\nhampers its practical implementation. To overcome this, we further propose a\nlow-complexity user scheduling policy based on characteristic analysis of the\nwireless channel to directly determine the user subset without iteration.\nExtensive experiments validate the superiority of the proposed method in terms\nof aggregation error and learning performance over existing approaches.",
    "clean_title": "integrated user scheduling and beam steering in over-the-air federated learning for mobile iot",
    "clean_abstract": "the rising popularity of internet of things iot has spurred technological advancements in mobile internet and interconnected systems. while offering flexible connectivity and intelligent applications across various domains, iot service providers must gather vast amounts of sensitive data from users, which nonetheless concomitantly raises concerns about privacy breaches. federated learning fl has emerged as a promising decentralized training paradigm to tackle this challenge. this work focuses on enhancing the aggregation efficiency of distributed local models by introducing over-the-air computation into the fl framework. due to radio resource scarcity in large-scale networks, only a subset of users can participate in each training round. this highlights the need for effective user scheduling and model transmission strategies to optimize communication efficiency and inference accuracy. to address this, we propose an integrated approach to user scheduling and receive beam steering, subject to constraints on the number of selected users and transmit power. leveraging the difference-of-convex technique, we decompose the primal non-convex optimization problem into two sub-problems, yielding an iterative solution. while effective, the computational load of the iterative method hampers its practical implementation. to overcome this, we further propose a low-complexity user scheduling policy based on characteristic analysis of the wireless channel to directly determine the user subset without iteration. extensive experiments validate the superiority of the proposed method in terms of aggregation error and learning performance over existing approaches.",
    "authors": [
      "Shengheng Liu",
      "Ningning Fu",
      "Zhonghao Zhang",
      "Yongming Huang",
      "Tony Q. S. Quek"
    ],
    "author_count": 5,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "keywords": [
      "user",
      "scheduling",
      "learning",
      "iot",
      "integrated",
      "beam",
      "over-the-air",
      "federated",
      "mobile",
      "internet"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00341v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00341v1"
  },
  {
    "id": "2508.00776v1",
    "title": "From Dynamic Programs to Greedy Algorithms",
    "abstract": "We show for several computational problems how classical greedy algorithms\nfor special cases can be derived in a simple way from dynamic programs for the\ngeneral case: interval scheduling (restricted to unit weights), knapsack\n(restricted to unit values), and shortest paths (restricted to nonnegative edge\nlengths). Conceptually, we repeatedly expand the Bellman equations underlying\nthe dynamic program and use straightforward monotonicity properties to figure\nout which terms yield the optimal value under the respective restrictions. The\napproach offers an alternative for developing these greedy algorithms in\nundergraduate algorithms courses and/or for arguing their correctness. In the\nsetting of interval scheduling, it elucidates the change in order from earliest\nstart time first for the memoized dynamic program to earliest finish time first\nfor the greedy algorithm.",
    "clean_title": "from dynamic programs to greedy algorithms",
    "clean_abstract": "we show for several computational problems how classical greedy algorithms for special cases can be derived in a simple way from dynamic programs for the general case: interval scheduling restricted to unit weights, knapsack restricted to unit values, and shortest paths restricted to nonnegative edge lengths. conceptually, we repeatedly expand the bellman equations underlying the dynamic program and use straightforward monotonicity properties to figure out which terms yield the optimal value under the respective restrictions. the approach offers an alternative for developing these greedy algorithms in undergraduate algorithms courses andor for arguing their correctness. in the setting of interval scheduling, it elucidates the change in order from earliest start time first for the memoized dynamic program to earliest finish time first for the greedy algorithm.",
    "authors": [
      "Dieter van Melkebeek"
    ],
    "author_count": 1,
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS",
    "keywords": [
      "dynamic",
      "greedy",
      "algorithms",
      "from",
      "restricted",
      "programs",
      "interval",
      "unit",
      "program",
      "earliest"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00776v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00776v1"
  },
  {
    "id": "2508.00276v1",
    "title": "Asymptotically Optimal Inapproximability of E$k$-SAT Reconfiguration",
    "abstract": "In the Maxmin E$k$-SAT Reconfiguration problem, we are given a satisfiable\n$k$-CNF formula $\\varphi$ where each clause contains exactly $k$ literals,\nalong with a pair of its satisfying assignments. The objective is transform one\nsatisfying assignment into the other by repeatedly flipping the value of a\nsingle variable, while maximizing the minimum fraction of satisfied clauses of\n$\\varphi$ throughout the transformation. In this paper, we demonstrate that the\noptimal approximation factor for Maxmin E$k$-SAT Reconfiguration is $1 -\n\\Theta\\left(\\frac{1}{k}\\right)$. On the algorithmic side, we develop a\ndeterministic $\\left(1-\\frac{1}{k-1}-\\frac{1}{k}\\right)$-factor approximation\nalgorithm for every $k \\geq 3$. On the hardness side, we show that it is\n$\\mathsf{PSPACE}$-hard to approximate this problem within a factor of\n$1-\\frac{1}{10k}$ for every sufficiently large $k$. Note that an\n``$\\mathsf{NP}$ analogue'' of Maxmin E$k$-SAT Reconfiguration is Max E$k$-SAT,\nwhose approximation threshold is $1-\\frac{1}{2^k}$ shown by H\\r{a}stad (JACM\n2001). To the best of our knowledge, this is the first reconfiguration problem\nwhose approximation threshold is (asymptotically) worse than that of its\n$\\mathsf{NP}$ analogue. To prove the hardness result, we introduce a new\n``non-monotone'' test, which is specially tailored to reconfiguration problems,\ndespite not being helpful in the PCP regime.",
    "clean_title": "asymptotically optimal inapproximability of ek-sat reconfiguration",
    "clean_abstract": "in the maxmin ek-sat reconfiguration problem, we are given a satisfiable k-cnf formula varphi where each clause contains exactly k literals, along with a pair of its satisfying assignments. the objective is transform one satisfying assignment into the other by repeatedly flipping the value of a single variable, while maximizing the minimum fraction of satisfied clauses of varphi throughout the transformation. in this paper, we demonstrate that the optimal approximation factor for maxmin ek-sat reconfiguration is 1 - thetaleftfrac1kright. on the algorithmic side, we develop a deterministic left1-frac1k-1-frac1kright-factor approximation algorithm for every k geq 3. on the hardness side, we show that it is mathsfpspace-hard to approximate this problem within a factor of 1-frac110k for every sufficiently large k. note that an mathsfnp analogue of maxmin ek-sat reconfiguration is max ek-sat, whose approximation threshold is 1-frac12k shown by hrastad jacm 2001. to the best of our knowledge, this is the first reconfiguration problem whose approximation threshold is asymptotically worse than that of its mathsfnp analogue. to prove the hardness result, we introduce a new non-monotone test, which is specially tailored to reconfiguration problems, despite not being helpful in the pcp regime.",
    "authors": [
      "Shuichi Hirahara",
      "Naoto Ohsaka"
    ],
    "author_count": 2,
    "categories": [
      "cs.CC",
      "cs.DM",
      "cs.DS"
    ],
    "primary_category": "cs.CC",
    "keywords": [
      "reconfiguration",
      "ek-sat",
      "approximation",
      "maxmin",
      "asymptotically",
      "optimal",
      "varphi",
      "its",
      "satisfying",
      "factor"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00276v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00276v1"
  },
  {
    "id": "2508.00055v1",
    "title": "Are controlled unitaries helpful?",
    "abstract": "Many quantum algorithms, to compute some property of a unitary $U$, require\naccess not just to $U$, but to $cU$, the unitary with a control qubit. We show\nthat having access to $cU$ does not help for a large class of quantum problems.\nFor a quantum circuit which uses $cU$ and $cU^\\dagger$ and outputs\n$|\\psi(U)\\rangle$, we show how to ``decontrol'' the circuit into one which uses\nonly $U$ and $U^\\dagger$ and outputs $|\\psi(\\varphi U)\\rangle$ for a uniformly\nrandom phase $\\varphi$, with a small amount of time and space overhead. When we\nonly care about the output state up to a global phase on $U$, then the\ndecontrolled circuit suffices. Stated differently, $cU$ is only helpful because\nit contains global phase information about $U$.\n  A version of our procedure is described in an appendix of Sheridan, Maslov,\nand Mosca [SMM09]. Our goal with this work is to popularize this result by\ngeneralizing it and investigating its implications, in order to counter\nnegative results in the literature which might lead one to believe that\ndecontrolling is not possible. As an application, we give a simple proof for\nthe existence of unitary ensembles which are pseudorandom under access to $U$,\n$U^\\dagger$, $cU$, and $cU^\\dagger$.",
    "clean_title": "are controlled unitaries helpful?",
    "clean_abstract": "many quantum algorithms, to compute some property of a unitary u, require access not just to u, but to cu, the unitary with a control qubit. we show that having access to cu does not help for a large class of quantum problems. for a quantum circuit which uses cu and cudagger and outputs psiurangle, we show how to decontrol the circuit into one which uses only u and udagger and outputs psivarphi urangle for a uniformly random phase varphi, with a small amount of time and space overhead. when we only care about the output state up to a global phase on u, then the decontrolled circuit suffices. stated differently, cu is only helpful because it contains global phase information about u. a version of our procedure is described in an appendix of sheridan, maslov, and mosca smm09. our goal with this work is to popularize this result by generalizing it and investigating its implications, in order to counter negative results in the literature which might lead one to believe that decontrolling is not possible. as an application, we give a simple proof for the existence of unitary ensembles which are pseudorandom under access to u, udagger, cu, and cudagger.",
    "authors": [
      "Ewin Tang",
      "John Wright"
    ],
    "author_count": 2,
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.DS"
    ],
    "primary_category": "quant-ph",
    "keywords": [
      "which",
      "quantum",
      "unitary",
      "access",
      "not",
      "circuit",
      "only",
      "phase",
      "cu,",
      "uses"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2508.00055v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00055v1"
  },
  {
    "id": "2507.23787v1",
    "title": "Amplitude amplification and estimation require inverses",
    "abstract": "We prove that the generic quantum speedups for brute-force search and\ncounting only hold when the process we apply them to can be efficiently\ninverted. The algorithms speeding up these problems, amplitude amplification\nand amplitude estimation, assume the ability to apply a state preparation\nunitary $U$ and its inverse $U^\\dagger$; we give problem instances based on\ntrace estimation where no algorithm which uses only $U$ beats the naive,\nquadratically slower approach. Our proof of this is simple and goes through the\ncompressed oracle method introduced by Zhandry. Since these two subroutines are\nresponsible for the ubiquity of the quadratic \"Grover\" speedup in quantum\nalgorithms, our result explains why such speedups are far harder to come by in\nthe settings of quantum learning, metrology, and sensing. In these settings,\n$U$ models the evolution of an experimental system, so implementing $U^\\dagger$\ncan be much harder -- tantamount to reversing time within the system. Our\nresult suggests a dichotomy: without inverse access, quantum speedups are\nscarce; with it, quantum speedups abound.",
    "clean_title": "amplitude amplification and estimation require inverses",
    "clean_abstract": "we prove that the generic quantum speedups for brute-force search and counting only hold when the process we apply them to can be efficiently inverted. the algorithms speeding up these problems, amplitude amplification and amplitude estimation, assume the ability to apply a state preparation unitary u and its inverse udagger; we give problem instances based on trace estimation where no algorithm which uses only u beats the naive, quadratically slower approach. our proof of this is simple and goes through the compressed oracle method introduced by zhandry. since these two subroutines are responsible for the ubiquity of the quadratic grover speedup in quantum algorithms, our result explains why such speedups are far harder to come by in the settings of quantum learning, metrology, and sensing. in these settings, u models the evolution of an experimental system, so implementing udagger can be much harder -- tantamount to reversing time within the system. our result suggests a dichotomy: without inverse access, quantum speedups are scarce; with it, quantum speedups abound.",
    "authors": [
      "Ewin Tang",
      "John Wright"
    ],
    "author_count": 2,
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.DS"
    ],
    "primary_category": "quant-ph",
    "keywords": [
      "quantum",
      "speedups",
      "amplitude",
      "amplification",
      "estimation",
      "only",
      "apply",
      "inverse",
      "result",
      "harder"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23787v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23787v1"
  },
  {
    "id": "2507.23659v1",
    "title": "Nyldon Factorization of Thue-Morse Words and Fibonacci Words",
    "abstract": "The Nyldon factorization is a string factorization that is a non-decreasing\nproduct of Nyldon words. Nyldon words and Nyldon factorizations are recently\ndefined combinatorial objects inspired by the well-known Lyndon words and\nLyndon factorizations. In this paper, we investigate the Nyldon factorization\nof several words. First, we fully characterize the Nyldon factorizations of the\n(finite) Fibonacci and the (finite) Thue-Morse words. Moreover, we show that\nthere exists a non-decreasing product of Nyldon words that is a factorization\nof the infinite Thue-Morse word.",
    "clean_title": "nyldon factorization of thue-morse words and fibonacci words",
    "clean_abstract": "the nyldon factorization is a string factorization that is a non-decreasing product of nyldon words. nyldon words and nyldon factorizations are recently defined combinatorial objects inspired by the well-known lyndon words and lyndon factorizations. in this paper, we investigate the nyldon factorization of several words. first, we fully characterize the nyldon factorizations of the finite fibonacci and the finite thue-morse words. moreover, we show that there exists a non-decreasing product of nyldon words that is a factorization of the infinite thue-morse word.",
    "authors": [
      "Kaisei Kishi",
      "Kazuki Kai",
      "Yuto Nakashima",
      "Shunsuke Inenaga",
      "Hideo Bannai"
    ],
    "author_count": 5,
    "categories": [
      "cs.DS",
      "cs.DM"
    ],
    "primary_category": "cs.DS",
    "keywords": [
      "nyldon",
      "factorization",
      "words",
      "thue-morse",
      "words.",
      "fibonacci",
      "non-decreasing",
      "product",
      "factorizations",
      "lyndon"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23659v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23659v1"
  },
  {
    "id": "2507.23539v1",
    "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions",
    "abstract": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.",
    "clean_title": "improved algorithms for kernel matrix-vector multiplication under sparsity assumptions",
    "clean_abstract": "motivated by the problem of fast processing of attention matrices, we study fast algorithms for computing matrix-vector products for asymmetric gaussian kernel matrices kin mathbbrntimes n. ks columns are indexed by a set of n keys k_1,k_2ldots, k_nin mathbbrd, rows by a set of n queries q_1,q_2,ldots,q_nin mathbbrd , and its i,j entry is k_ij  e-q_i-k_j_222sigma2 for some bandwidth parameter sigma0. given a vector xin mathbbrn and error parameter epsilon0, our task is to output a yin mathbbrn such that kx-y_2leq epsilon x_2 in time subquadratic in n and linear in d. our algorithms rely on the following modelling assumption about the matrices k: the sum of the entries of k scales linearly in n, as opposed to worst case quadratic growth. we validate this assumption experimentally, for gaussian kernel matrices encountered in various settings such as fast attention computation in llms. we obtain the first subquadratic-time algorithm that works under this assumption, for unrestricted vectors.",
    "authors": [
      "Piotr Indyk",
      "Michael Kapralov",
      "Kshiteej Sheth",
      "Tal Wagner"
    ],
    "author_count": 4,
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "algorithms",
      "kernel",
      "fast",
      "matrices",
      "matrix-vector",
      "under",
      "attention",
      "gaussian",
      "set",
      "parameter"
    ],
    "published_date": "2025-07-31",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.23539v1",
    "pdf_url": "http://arxiv.org/pdf/2507.23539v1"
  },
  {
    "id": "2508.00773v1",
    "title": "Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery",
    "abstract": "Cardiorespiratory coupling (CRC) captures the dynamic interaction between the\ncardiac and respiratory systems--an interaction strengthened by physical\nexercise and linked to improved physiological function. We examined CRC at high\naltitude in two states, rest and post-exercise recovery, and found significant\ndifferences (p < 0.05). Quantitative analysis revealed that recovery involved\nmore frequent yet less stable episodes of synchronization between respiration\nand pulse. Furthermore, we explored the feasibility of non-contact CRC\nmeasurement with remote photoplethysmography (rPPG), observing a strong\ncorrelation with oximeter-based metrics (Pearson r = 0.96). These findings\nhighlight the potential of CRC as a sensitive marker for autonomic regulation\nand its future application in contactless monitoring. Source code is available\nat GitHub: https://github.com/McJackTang/CRC.",
    "clean_title": "contact sensors to remote cameras: quantifying cardiorespiratory coupling in high-altitude exercise recovery",
    "clean_abstract": "cardiorespiratory coupling crc captures the dynamic interaction between the cardiac and respiratory systems--an interaction strengthened by physical exercise and linked to improved physiological function. we examined crc at high altitude in two states, rest and post-exercise recovery, and found significant differences p  0.05. quantitative analysis revealed that recovery involved more frequent yet less stable episodes of synchronization between respiration and pulse. furthermore, we explored the feasibility of non-contact crc measurement with remote photoplethysmography rppg, observing a strong correlation with oximeter-based metrics pearson r  0.96. these findings highlight the potential of crc as a sensitive marker for autonomic regulation and its future application in contactless monitoring. source code is available at github: https:github.commcjacktangcrc.",
    "authors": [
      "Jiankai Tang",
      "Meng Kang",
      "Yiru Zhang",
      "Kegang Wang",
      "Daniel Mcduff",
      "Xin Liu",
      "Yuanchun Shi",
      "Yuntao Wang"
    ],
    "author_count": 8,
    "categories": [
      "cs.CE",
      "cs.HC"
    ],
    "primary_category": "cs.CE",
    "keywords": [
      "crc",
      "remote",
      "cardiorespiratory",
      "coupling",
      "exercise",
      "recovery",
      "interaction",
      "between",
      "contact",
      "sensors"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00773v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00773v1"
  },
  {
    "id": "2508.00737v1",
    "title": "How LLMs are Shaping the Future of Virtual Reality",
    "abstract": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.",
    "clean_title": "how llms are shaping the future of virtual reality",
    "clean_abstract": "the integration of large language models llms into virtual reality vr games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. this paper presents a comprehensive review of recent research at the intersection of llms and vr, examining how these models are transforming narrative generation, non-player character npc interactions, accessibility, personalization, and game mastering. drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent npcs and procedurally generated storytelling to ai-driven adaptive systems and inclusive gameplay interfaces. we also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. our findings highlight that while llms significantly enhance realism, creativity, and user engagement in vr environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid ai architectures, and ethical safeguards. the paper concludes by outlining future research directions in multimodal ai, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive vr systems.",
    "authors": [
      "Seda zkaya",
      "Santiago Berrezueta-Guzman",
      "Stefan Wagner"
    ],
    "author_count": 3,
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "keywords": [
      "llms",
      "intelligent",
      "how",
      "future",
      "virtual",
      "reality",
      "models",
      "design",
      "research",
      "from"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00737v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00737v1"
  },
  {
    "id": "2508.00723v1",
    "title": "Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption",
    "abstract": "Growing excitement around deploying AI across various domains calls for a\ncareful assessment of how human decision-makers interact with AI-powered\nsystems. In particular, it is essential to understand when decision-makers\nvoluntarily choose to consult AI tools, which we term decision-maker adoption.\nWe interviewed experts across four domains -- medicine, law, journalism, and\nthe public sector -- to explore current AI use cases and perceptions of\nadoption. From these interviews, we identify key factors that shape\ndecision-maker adoption of AI tools: the decision-maker's background,\nperceptions of the AI, consequences for the decision-maker, and perceived\nimplications for other stakeholders. We translate these factors into an AI\nadoption sheet to analyze how decision-makers approach adoption choices through\ncomparative, cross-domain case studies, highlighting how our factors help\nexplain inter-domain differences in adoption. Our findings offer practical\nguidance for supporting the responsible and context-aware deployment of AI by\nbetter accounting for the decision-maker's perspective.",
    "clean_title": "why do decision makers not use ai? a cross-domain analysis of factors impacting ai adoption",
    "clean_abstract": "growing excitement around deploying ai across various domains calls for a careful assessment of how human decision-makers interact with ai-powered systems. in particular, it is essential to understand when decision-makers voluntarily choose to consult ai tools, which we term decision-maker adoption. we interviewed experts across four domains -- medicine, law, journalism, and the public sector -- to explore current ai use cases and perceptions of adoption. from these interviews, we identify key factors that shape decision-maker adoption of ai tools: the decision-makers background, perceptions of the ai, consequences for the decision-maker, and perceived implications for other stakeholders. we translate these factors into an ai adoption sheet to analyze how decision-makers approach adoption choices through comparative, cross-domain case studies, highlighting how our factors help explain inter-domain differences in adoption. our findings offer practical guidance for supporting the responsible and context-aware deployment of ai by better accounting for the decision-makers perspective.",
    "authors": [
      "Rebecca Yu",
      "Valerie Chen",
      "Ameet Talwalkar",
      "Hoda Heidari"
    ],
    "author_count": 4,
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "keywords": [
      "decision-makers",
      "factors",
      "adoption",
      "how",
      "adoption.",
      "use",
      "cross-domain",
      "across",
      "domains",
      "decision-maker"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00723v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00723v1"
  },
  {
    "id": "2508.00674v1",
    "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations",
    "abstract": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.",
    "clean_title": "context-aware visualization for explainable ai recommendations in social media: a vision for user-aligned explanations",
    "clean_abstract": "social media platforms today strive to improve user experience through ai recommendations, yet the value of such recommendations vanishes as users do not understand the reasons behind them. this issue arises because explainability in social media is general and lacks alignment with user-specific needs. in this vision paper, we outline a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods. the proposed system is framed by the variety of user needs and contexts, showing explanations in different visualized forms, including a technically detailed version for ai experts and a simplified one for lay users. our framework is the first to jointly adapt explanation style visual vs. numeric and granularity expert vs. lay inside a single pipeline. a public pilot with 30 x users will validate its impact on decision-making and trust.",
    "authors": [
      "Banan Alkhateeb",
      "Ellis Solaiman"
    ],
    "author_count": 2,
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "keywords": [
      "explanation",
      "social",
      "context-aware",
      "recommendations",
      "vision",
      "explanations",
      "media",
      "user",
      "users",
      "visual"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00674v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00674v1"
  },
  {
    "id": "2508.00665v1",
    "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI",
    "abstract": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.",
    "clean_title": "transparent adaptive learning via data-centric multimodal explainable ai",
    "clean_abstract": "artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. yet many of these systems lack transparency, offering limited insight into how decisions are made. most explainable ai xai techniques focus on technical outputs but neglect user roles and comprehension. this paper proposes a hybrid framework that integrates traditional xai techniques with generative ai models and user personalisation to generate multimodal, personalised explanations tailored to user needs. we redefine explainability as a dynamic communication process tailored to user roles and learning goals. we outline the frameworks design, key xai limitations in education, and research directions on accuracy, fairness, and personalisation. our aim is to move towards explainable ai that enhances transparency while supporting user-centred experiences.",
    "authors": [
      "Maryam Mosleh",
      "Marie Devlin",
      "Ellis Solaiman"
    ],
    "author_count": 3,
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "keywords": [
      "learning",
      "user",
      "explainable",
      "xai",
      "adaptive",
      "systems",
      "experiences.",
      "techniques",
      "roles",
      "tailored"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00665v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00665v1"
  },
  {
    "id": "2508.00652v1",
    "title": "The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech",
    "abstract": "Pervasive voice interaction enables deceptive patterns through subtle voice\ncharacteristics, yet empirical investigation into this manipulation lags\nbehind, especially within major non-English language contexts. Addressing this\ngap, our study presents the first systematic investigation into voice\ncharacteristic-based dark patterns employing female synthetic voices in\nMandarin Chinese. This focus is crucial given the prevalence of female personas\nin commercial assistants and the prosodic significance in the Chinese language.\nGuided by the conceptual framework identifying key influencing factors, we\nsystematically evaluate effectiveness variations by manipulating voice\ncharacteristics (five characteristics, three intensities) across different\nscenarios (shopping vs. question-answering) with different commercial aims. A\npreliminary study (N=24) validated the experimental materials and the main\nstudy (N=36) revealed significant behavioral manipulation (up to +2027.6%).\nCrucially, the analysis showed that effectiveness varied significantly with\nvoice characteristics and scenario, mediated by user perception (of tone,\nintonation, timbre) and user demographics (individual preferences, though\nlimited demographic impact). These interconnected findings offer evidence-based\ninsights for ethical design.",
    "clean_title": "the manipulative power of voice characteristics: investigating deceptive patterns in mandarin chinese female synthetic speech",
    "clean_abstract": "pervasive voice interaction enables deceptive patterns through subtle voice characteristics, yet empirical investigation into this manipulation lags behind, especially within major non-english language contexts. addressing this gap, our study presents the first systematic investigation into voice characteristic-based dark patterns employing female synthetic voices in mandarin chinese. this focus is crucial given the prevalence of female personas in commercial assistants and the prosodic significance in the chinese language. guided by the conceptual framework identifying key influencing factors, we systematically evaluate effectiveness variations by manipulating voice characteristics five characteristics, three intensities across different scenarios shopping vs. question-answering with different commercial aims. a preliminary study n24 validated the experimental materials and the main study n36 revealed significant behavioral manipulation up to 2027.6. crucially, the analysis showed that effectiveness varied significantly with voice characteristics and scenario, mediated by user perception of tone, intonation, timbre and user demographics individual preferences, though limited demographic impact. these interconnected findings offer evidence-based insights for ethical design.",
    "authors": [
      "Shuning Zhang",
      "Han Chen",
      "Yabo Wang",
      "Yiqun Xu",
      "Jiaqi Bai",
      "Yuanyuan Wu",
      "Shixuan Li",
      "Xin Yi",
      "Chunhui Wang",
      "Hewu Li"
    ],
    "author_count": 10,
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC",
    "keywords": [
      "voice",
      "patterns",
      "female",
      "study",
      "deceptive",
      "mandarin",
      "chinese",
      "synthetic",
      "characteristics,",
      "investigation"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00652v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00652v1"
  },
  {
    "id": "2508.00751v1",
    "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking",
    "abstract": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.",
    "clean_title": "harnessing the power of interleaving and counterfactual evaluation for airbnb search ranking",
    "clean_abstract": "evaluation plays a crucial role in the development of ranking algorithms on search and recommender systems. it enables online platforms to create user-friendly features that drive commercial success in a steady and effective manner. the online environment is particularly conducive to applying causal inference techniques, such as randomized controlled experiments known as ab test, which are often more challenging to implement in fields like medicine and public policy. however, businesses face unique challenges when it comes to effective ab test. specifically, achieving sufficient statistical power for conversion-based metrics can be time-consuming, especially for significant purchases like booking accommodations. while offline evaluations are quicker and more cost-effective, they often lack accuracy and are inadequate for selecting candidates for ab test. to address these challenges, we developed interleaving and counterfactual evaluation methods to facilitate rapid online assessments for identifying the most promising candidates for ab tests. our approach not only increased the sensitivity of experiments by a factor of up to 100 depending on the approach and metrics compared to traditional ab testing but also streamlined the experimental process. the practical insights gained from usage in production can also benefit organizations with similar interests.",
    "authors": [
      "Qing Zhang",
      "Alex Deng",
      "Michelle Du",
      "Huiji Gao",
      "Liwei He",
      "Sanjeev Katariya"
    ],
    "author_count": 6,
    "categories": [
      "cs.IR",
      "cs.AI",
      "H.3; G.3"
    ],
    "primary_category": "cs.IR",
    "keywords": [
      "evaluation",
      "online",
      "power",
      "interleaving",
      "counterfactual",
      "search",
      "ranking",
      "effective",
      "experiments",
      "often"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00751v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00751v1"
  },
  {
    "id": "2508.00710v1",
    "title": "Experimental Evaluation of Dynamic Topic Modeling Algorithms",
    "abstract": "The amount of text generated daily on social media is gigantic and analyzing\nthis text is useful for many purposes. To understand what lies beneath a huge\namount of text, we need dependable and effective computing techniques from\nself-powered topic models. Nevertheless, there are currently relatively few\nthorough quantitative comparisons between these models. In this study, we\ncompare these models and propose an assessment metric that documents how the\ntopics change in time.",
    "clean_title": "experimental evaluation of dynamic topic modeling algorithms",
    "clean_abstract": "the amount of text generated daily on social media is gigantic and analyzing this text is useful for many purposes. to understand what lies beneath a huge amount of text, we need dependable and effective computing techniques from self-powered topic models. nevertheless, there are currently relatively few thorough quantitative comparisons between these models. in this study, we compare these models and propose an assessment metric that documents how the topics change in time.",
    "authors": [
      "Ngozichukwuka Onah",
      "Nadine Steinmetz",
      "Hani Al-Sayeh",
      "Kai-Uwe Sattler"
    ],
    "author_count": 4,
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR",
    "keywords": [
      "topic",
      "amount",
      "text",
      "models.",
      "experimental",
      "evaluation",
      "dynamic",
      "modeling",
      "algorithms",
      "generated"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00710v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00710v1"
  },
  {
    "id": "2508.00709v1",
    "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System",
    "abstract": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
    "clean_title": "nyayarag: realistic legal judgment prediction with rag under the indian common law system",
    "clean_abstract": "legal judgment prediction ljp has emerged as a key area in ai for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. while previous approaches in the indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. in this work, we propose nyayarag, a retrieval-augmented generation rag framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. nyayarag evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the indian legal system. we assess performance across various input configurations using both standard lexical and semantic metrics as well as llm-based evaluators such as g-eval. our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.",
    "authors": [
      "Shubham Kumar Nigam",
      "Balaramamahanthi Deepak Patnaik",
      "Shivam Mishra",
      "Ajay Varghese Thomas",
      "Noel Shallum",
      "Kripabandhu Ghosh",
      "Arnab Bhattacharya"
    ],
    "author_count": 7,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "legal",
      "indian",
      "realistic",
      "judgment",
      "prediction",
      "rag",
      "common",
      "law",
      "judicial",
      "case"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00709v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00709v1"
  },
  {
    "id": "2508.00679v1",
    "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries",
    "abstract": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.",
    "clean_title": "segment first, retrieve better: realistic legal search via rhetorical role-based queries",
    "clean_abstract": "legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. however, the growing complexity and volume of legal documents challenge traditional retrieval methods. traceretriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. our pipeline integrates bm25, vector database, and cross-encoder models, combining initial results through reciprocal rank fusion before final re-ranking. rhetorical annotations are generated using a hierarchical bilstm crf classifier trained on indian judgments. evaluated on il-pcr and coliee 2025 datasets, traceretriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.",
    "authors": [
      "Shubham Kumar Nigam",
      "Tanmay Dubey",
      "Noel Shallum",
      "Arnab Bhattacharya"
    ],
    "author_count": 4,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "keywords": [
      "legal",
      "search",
      "retrieval",
      "rhetorical",
      "precedent",
      "growing",
      "volume",
      "traceretriever",
      "case",
      "only"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00679v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00679v1"
  },
  {
    "id": "2508.00589v1",
    "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving",
    "abstract": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.",
    "clean_title": "context-based motion retrieval using open vocabulary methods for autonomous driving",
    "clean_abstract": "autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by vulnerable road users vrus. identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. to support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. our method combines skinned multi-person linear smpl-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. our approach enables the scalable retrieval of human behavior and their context through text queries. this work also introduces our dataset waymoco, an extension of the waymo open dataset. it contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth smpl sequences and corresponding image data. our approach outperforms state-of-the-art models by up to 27.5 accuracy in motion-context retrieval, when evaluated on the waymoco dataset.",
    "authors": [
      "Stefan Englmeier",
      "Max A. Bttner",
      "Katharina Winter",
      "Fabian B. Flohr"
    ],
    "author_count": 4,
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.IR",
      "cs.RO",
      "68T45, 68P20, 68T10, 68T50, 68T07, 68T40",
      "I.2.10; I.4.8; I.2.9; H.3.3"
    ],
    "primary_category": "cs.CV",
    "keywords": [
      "motion",
      "driving",
      "retrieval",
      "autonomous",
      "behavior",
      "open",
      "systems",
      "scenarios,",
      "datasets",
      "evaluation"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00589v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00589v1"
  },
  {
    "id": "2508.00579v1",
    "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval",
    "abstract": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
    "clean_title": "mmrag-docqa: a multi-modal retrieval-augmented generation method for document question-answering with hierarchical index and multi-granularity retrieval",
    "clean_abstract": "the multi-modal long-context document question-answering task aims to locate and integrate multi-modal evidences such as texts, tables, charts, images, and layouts distributed across multiple pages, for question understanding and answer generation. the existing methods can be categorized into large vision-language model lvlm-based and retrieval-augmented generation rag-based methods. however, the former were susceptible to hallucinations, while the latter struggled for inter-modal disconnection and cross-page fragmentation. to address these challenges, a novel multi-modal rag model, named mmrag-docqa, was proposed, leveraging both textual and visual information across long-range pages to facilitate accurate question answering. a hierarchical indexing method with the integration of flattened in-page chunks and topological cross-page chunks was designed to jointly establish in-page multi-modal associations and long-distance cross-page dependencies. by means of joint similarity evaluation and large language model llm-based re-ranking, a multi-granularity semantic retrieval method, including the page-level parent page retrieval and document-level summary retrieval, was proposed to foster multi-modal evidence connection and long-distance evidence integration and reasoning. experimental results performed on public datasets, mmlongbench-doc and longdocurl, demonstrated the superiority of our mmrag-docqa method in understanding and answering modality-rich and multi-page documents.",
    "authors": [
      "Ziyu Gong",
      "Yihua Huang",
      "Chengcheng Mai"
    ],
    "author_count": 3,
    "categories": [
      "cs.MM",
      "cs.IR"
    ],
    "primary_category": "cs.MM",
    "keywords": [
      "multi-modal",
      "retrieval",
      "cross-page",
      "retrieval-augmented",
      "generation",
      "document",
      "question-answering",
      "hierarchical",
      "multi-granularity",
      "across"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00579v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00579v1"
  },
  {
    "id": "2508.00792v1",
    "title": "Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype",
    "abstract": "The Data Movement Manager (DMM) is a prototype interface that connects CERN's\ndata management software, Rucio, with the Sofware-Defined Networking (SDN)\nservice SENSE by ESNet. It enables SDN-enabled high-energy physics data flows\nusing the existing worldwide LHC computing grid infrastructure. A key feature\nof DMM is transfer priority-based bandwidth allocation, optimizing network\nusage. Additionally, it provides fine-grained monitoring of underperforming\nflows by leveraging end-to-end data flow monitoring. This is achieved through\naccess to host-level (network interface) throughput metrics and transfer-tool\n(FTS) data transfer job-level metrics. This paper details the design and\nimplementation of DMM.",
    "clean_title": "data movement manager dmm for the sense-rucio interoperation prototype",
    "clean_abstract": "the data movement manager dmm is a prototype interface that connects cerns data management software, rucio, with the sofware-defined networking sdn service sense by esnet. it enables sdn-enabled high-energy physics data flows using the existing worldwide lhc computing grid infrastructure. a key feature of dmm is transfer priority-based bandwidth allocation, optimizing network usage. additionally, it provides fine-grained monitoring of underperforming flows by leveraging end-to-end data flow monitoring. this is achieved through access to host-level network interface throughput metrics and transfer-tool fts data transfer job-level metrics. this paper details the design and implementation of dmm.",
    "authors": [
      "Aashay Arora",
      "Diego Davila",
      "Jonathan Guiang",
      "Frank Wrthwein",
      "Harvey Newman",
      "Justas Balcas",
      "Tom Lehman",
      "Xi Yang"
    ],
    "author_count": 8,
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "data",
      "dmm",
      "movement",
      "manager",
      "prototype",
      "interface",
      "flows",
      "transfer",
      "network",
      "sense-rucio"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00792v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00792v1"
  },
  {
    "id": "2508.00735v1",
    "title": "Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE",
    "abstract": "IP fragmentation and TCP segmentation allow for splitting large data packets\ninto smaller ones, e.g., for transmission across network links of limited\ncapacity. These mechanisms permit complete or partial overlaps with different\ndata on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,\ni.e., the data chunk preferences that depend on the overlap types, differ\nacross protocol implementations. This leads to vulnerabilities, as NIDSes may\ninterpret the packet differently from the monitored host OSes. Some NIDSes,\nsuch as Suricata or Snort, can be configured so that their policies are\nconsistent with the monitored OSes. The first contribution of the paper is\nPYROLYSE, an audit tool that exhaustively tests and describes the reassembly\npolicies of various IP and TCP implementation types. This tool ensures that\nimplementations reassemble overlapping chunk sequences without errors. The\nsecond contribution is the analysis of PYROLYSE artifacts. We first show that\nthe reassembly policies are much more diverse than previously thought. Indeed,\nby testing all the overlap possibilities for n <= 3 test case chunks and\ndifferent testing scenarios, we observe from 14 to 20 different behaviors out\nof 23 tested implementations depending on the protocol. Second, we report eight\nerrors impacting one OS, two NIDSes, and two embedded stacks, which can lead to\nsecurity issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was\nassigned to a NIDS error. Finally, we show that implemented IP and TCP policies\nobtained through chunk pair testing are usually inconsistent with the observed\ntriplet reassemblies. Therefore, contrarily to what they currently do, NIDSes\nor other network traffic analysis tools should not apply n = 2 pair policies\nwhen the number of overlapping chunks exceeds two.",
    "clean_title": "overlapping ipv4, ipv6, and tcp data: exploring errors, test case context and multiple overlaps inside network stacks and nidses with pyrolyse",
    "clean_abstract": "ip fragmentation and tcp segmentation allow for splitting large data packets into smaller ones, e.g., for transmission across network links of limited capacity. these mechanisms permit complete or partial overlaps with different data on the overlapping portions. ipv4, ipv6, and tcp reassembly policies, i.e., the data chunk preferences that depend on the overlap types, differ across protocol implementations. this leads to vulnerabilities, as nidses may interpret the packet differently from the monitored host oses. some nidses, such as suricata or snort, can be configured so that their policies are consistent with the monitored oses. the first contribution of the paper is pyrolyse, an audit tool that exhaustively tests and describes the reassembly policies of various ip and tcp implementation types. this tool ensures that implementations reassemble overlapping chunk sequences without errors. the second contribution is the analysis of pyrolyse artifacts. we first show that the reassembly policies are much more diverse than previously thought. indeed, by testing all the overlap possibilities for n  3 test case chunks and different testing scenarios, we observe from 14 to 20 different behaviors out of 23 tested implementations depending on the protocol. second, we report eight errors impacting one os, two nidses, and two embedded stacks, which can lead to security issues such as nids pattern-matching bypass or dos attacks. a cve was assigned to a nids error. finally, we show that implemented ip and tcp policies obtained through chunk pair testing are usually inconsistent with the observed triplet reassemblies. therefore, contrarily to what they currently do, nidses or other network traffic analysis tools should not apply n  2 pair policies when the number of overlapping chunks exceeds two.",
    "authors": [
      "Lucas Aubard",
      "Johan Mazel",
      "Gilles Guette",
      "Pierre Chifflier"
    ],
    "author_count": 4,
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "tcp",
      "policies",
      "overlapping",
      "network",
      "nidses",
      "data",
      "different",
      "reassembly",
      "chunk",
      "testing"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00735v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00735v1"
  },
  {
    "id": "2508.00715v1",
    "title": "Deep Joint Source-Channel Coding for Small Satellite Applications",
    "abstract": "Small satellites used for Earth observation generate vast amounts of\nhigh-dimensional data, but their operation in low Earth orbit creates a\nsignificant communication bottleneck due to limited contact times and harsh,\nvarying channel conditions. While deep joint source-channel coding (DJSCC) has\nemerged as a promising technique, its practical application to the complex\nsatellite environment remains an open question. This paper presents a\ncomprehensive DJSCC framework tailored for satellite communications. We first\nestablish a basic system, DJSCC-SAT, and integrate a realistic, multi-state\nstatistical channel model to guide its training and evaluation. To overcome the\nimpracticality of using separate models for every channel condition, we then\nintroduce an adaptable architecture, ADJSCC-SAT, which leverages attention\nmodules to allow a single neural network to adjust to a wide range of channel\nstates with minimal overhead. Through extensive evaluation on Sentinel-2\nmulti-spectral data, we demonstrate that our adaptable approach achieves\nperformance comparable to using multiple specialized networks while\nsignificantly reducing model storage requirements. Furthermore, the adaptable\nmodel shows enhanced robustness to channel estimation errors, outperforming the\nnon-adaptable baseline. The proposed framework is a practical and efficient\nstep toward deploying robust, adaptive DJSCC systems for real-world satellite\nmissions.",
    "clean_title": "deep joint source-channel coding for small satellite applications",
    "clean_abstract": "small satellites used for earth observation generate vast amounts of high-dimensional data, but their operation in low earth orbit creates a significant communication bottleneck due to limited contact times and harsh, varying channel conditions. while deep joint source-channel coding djscc has emerged as a promising technique, its practical application to the complex satellite environment remains an open question. this paper presents a comprehensive djscc framework tailored for satellite communications. we first establish a basic system, djscc-sat, and integrate a realistic, multi-state statistical channel model to guide its training and evaluation. to overcome the impracticality of using separate models for every channel condition, we then introduce an adaptable architecture, adjscc-sat, which leverages attention modules to allow a single neural network to adjust to a wide range of channel states with minimal overhead. through extensive evaluation on sentinel-2 multi-spectral data, we demonstrate that our adaptable approach achieves performance comparable to using multiple specialized networks while significantly reducing model storage requirements. furthermore, the adaptable model shows enhanced robustness to channel estimation errors, outperforming the non-adaptable baseline. the proposed framework is a practical and efficient step toward deploying robust, adaptive djscc systems for real-world satellite missions.",
    "authors": [
      "Olga Kondrateva",
      "Grace Li Zhang",
      "Julian Zobel",
      "Bjrn Scheuermann",
      "Stefan Dietzel"
    ],
    "author_count": 5,
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "channel",
      "satellite",
      "djscc",
      "model",
      "adaptable",
      "deep",
      "joint",
      "source-channel",
      "coding",
      "small"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00715v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00715v1"
  },
  {
    "id": "2508.00688v1",
    "title": "Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience",
    "abstract": "Heterogeneous marine-aerial swarm networks encounter substantial difficulties\ndue to targeted communication disruptions and structural weaknesses in\nadversarial environments. This paper proposes a two-step framework to\nstrengthen the network's resilience. Specifically, our framework combines the\nnode prioritization based on criticality with multi-objective topology\noptimization. First, we design a three-layer architecture to represent\nstructural, communication, and task dependencies of the swarm networks. Then,\nwe introduce the SurBi-Ranking method, which utilizes graph convolutional\nnetworks, to dynamically evaluate and rank the criticality of nodes and edges\nin real time. Next, we apply the NSGA-III algorithm to optimize the network\ntopology, aiming to balance communication efficiency, global connectivity, and\nmission success rate. Experiments demonstrate that compared to traditional\nmethods like K-Shell, our SurBi-Ranking method identifies critical nodes and\nedges with greater accuracy, as deliberate attacks on these components cause\nmore significant connectivity degradation. Furthermore, our optimization\napproach, when prioritizing SurBi-Ranked critical components under attack,\nreduces the natural connectivity degradation by around 30%, achieves higher\nmission success rates, and incurs lower communication reconfiguration costs,\nensuring sustained connectivity and mission effectiveness across multi-phase\noperations.",
    "clean_title": "criticality-based dynamic topology optimization for enhancing aerial-marine swarm resilience",
    "clean_abstract": "heterogeneous marine-aerial swarm networks encounter substantial difficulties due to targeted communication disruptions and structural weaknesses in adversarial environments. this paper proposes a two-step framework to strengthen the networks resilience. specifically, our framework combines the node prioritization based on criticality with multi-objective topology optimization. first, we design a three-layer architecture to represent structural, communication, and task dependencies of the swarm networks. then, we introduce the surbi-ranking method, which utilizes graph convolutional networks, to dynamically evaluate and rank the criticality of nodes and edges in real time. next, we apply the nsga-iii algorithm to optimize the network topology, aiming to balance communication efficiency, global connectivity, and mission success rate. experiments demonstrate that compared to traditional methods like k-shell, our surbi-ranking method identifies critical nodes and edges with greater accuracy, as deliberate attacks on these components cause more significant connectivity degradation. furthermore, our optimization approach, when prioritizing surbi-ranked critical components under attack, reduces the natural connectivity degradation by around 30, achieves higher mission success rates, and incurs lower communication reconfiguration costs, ensuring sustained connectivity and mission effectiveness across multi-phase operations.",
    "authors": [
      "Ruiyang Huang",
      "Haocheng Wang",
      "Yixuan Shen",
      "Ning Gao",
      "Qiang Ni",
      "Shi Jin",
      "Yifan Wu"
    ],
    "author_count": 7,
    "categories": [
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "swarm",
      "communication",
      "mission",
      "connectivity",
      "topology",
      "optimization",
      "networks",
      "framework",
      "criticality",
      "surbi-ranking"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00688v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00688v1"
  },
  {
    "id": "2508.00629v1",
    "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach",
    "abstract": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
    "clean_title": "energy-aware cpu orchestration in o-ran: a dapp-driven lightweight approach",
    "clean_abstract": "the transition toward softwarized radio access networks rans, driven by the open ran o-ran paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. however, this shift introduces new challenges in managing cpu resources efficiently under strict real-time constraints. in particular, the interplay between latency-sensitive ran workloads and general-purpose operating system os schedulers often leads to sub-optimal performance and unnecessary energy consumption. this work proposes a lightweight, programmable distributed application dapp deployed at the distributed unit du level to dynamically orchestrate cpu usage. the dapp operates in closed loop with the os, leveraging thread-level telemetry like context switches, instructions per cycle ipc, and cache metrics, to adapt cpu thread affinity, core isolation, and frequency scaling in real time. unlike existing solutions, it requires no access to proprietary ran software, hardware-specific features, or kernel modifications. fully compliant with the o-ran architecture and agnostic to the underlying ran stack, the proposed solution introduces negligible overhead while improving energy efficiency and cpu utilization. experimental results using a commercial-grade srsran deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dapps for fine-grained resource control in next-generation networks",
    "authors": [
      "Francisco Crespo",
      "Javier Villegas",
      "Carlos Baena",
      "Eduardo Baena",
      "Sergio Fortes",
      "Raquel Barco"
    ],
    "author_count": 6,
    "categories": [
      "cs.NI",
      "cs.OS",
      "cs.PF"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "cpu",
      "ran",
      "access",
      "networks",
      "o-ran",
      "introduces",
      "real-time",
      "energy",
      "distributed",
      "dapp"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00629v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00629v1"
  },
  {
    "id": "2508.00616v1",
    "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications",
    "abstract": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
    "clean_title": "joint association and phase shifts design for uav-mounted stacked intelligent metasurfaces-assisted communications",
    "clean_abstract": "stacked intelligent metasurfaces sims have emerged as a promising technology for realizing wave-domain signal processing, while the fixed sims will limit the communication performance of the system compared to the mobile sims. in this work, we consider a uav-mounted sims uav-sims assisted communication system, where uavs as base stations bss can cache the data processed by sims, and also as mobile vehicles flexibly deploy sims to enhance the communication performance. to this end, we formulate a uav-sim-based joint optimization problem usbjop to comprehensively consider the association between uav-sims and users, the locations of uav-sims, and the phase shifts of uav-sims, aiming to maximize the network capacity. due to the non-convexity and np-hardness of usbjop, we decompose it into three sub-optimization problems, which are the association between uav-sims and users optimization problem auuop, the uav location optimization problem ulop, and the uav-sim phase shifts optimization problem uspsop. then, these three sub-optimization problems are solved by an alternating optimization ao strategy. specifically, auuop and ulop are transformed to a convex form and then solved by the cvx tool, while we employ a layer-by-layer iterative optimization method for uspsop. simulation results verify the effectiveness of the proposed strategy under different simulation setups.",
    "authors": [
      "Mingzhe Fan",
      "Geng Sun",
      "Hongyang Pan",
      "Jiacheng Wang",
      "Jiancheng An",
      "Hongyang Du",
      "Chau Yuen"
    ],
    "author_count": 7,
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "optimization",
      "sims",
      "problem",
      "association",
      "phase",
      "shifts",
      "communication",
      "uav-sims",
      "joint",
      "uav-mounted"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00616v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00616v1"
  },
  {
    "id": "2508.00629v1",
    "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach",
    "abstract": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
    "clean_title": "energy-aware cpu orchestration in o-ran: a dapp-driven lightweight approach",
    "clean_abstract": "the transition toward softwarized radio access networks rans, driven by the open ran o-ran paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. however, this shift introduces new challenges in managing cpu resources efficiently under strict real-time constraints. in particular, the interplay between latency-sensitive ran workloads and general-purpose operating system os schedulers often leads to sub-optimal performance and unnecessary energy consumption. this work proposes a lightweight, programmable distributed application dapp deployed at the distributed unit du level to dynamically orchestrate cpu usage. the dapp operates in closed loop with the os, leveraging thread-level telemetry like context switches, instructions per cycle ipc, and cache metrics, to adapt cpu thread affinity, core isolation, and frequency scaling in real time. unlike existing solutions, it requires no access to proprietary ran software, hardware-specific features, or kernel modifications. fully compliant with the o-ran architecture and agnostic to the underlying ran stack, the proposed solution introduces negligible overhead while improving energy efficiency and cpu utilization. experimental results using a commercial-grade srsran deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dapps for fine-grained resource control in next-generation networks",
    "authors": [
      "Francisco Crespo",
      "Javier Villegas",
      "Carlos Baena",
      "Eduardo Baena",
      "Sergio Fortes",
      "Raquel Barco"
    ],
    "author_count": 6,
    "categories": [
      "cs.NI",
      "cs.OS",
      "cs.PF"
    ],
    "primary_category": "cs.NI",
    "keywords": [
      "cpu",
      "ran",
      "access",
      "networks",
      "o-ran",
      "introduces",
      "real-time",
      "energy",
      "distributed",
      "dapp"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00629v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00629v1"
  },
  {
    "id": "2508.00604v1",
    "title": "Composable OS Kernel Architectures for Autonomous Intelligence",
    "abstract": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications.",
    "clean_title": "composable os kernel architectures for autonomous intelligence",
    "clean_abstract": "as intelligent systems permeate edge devices, cloud infrastructure, and embedded real-time environments, this research proposes a new os kernel architecture for intelligent systems, transforming kernels from static resource managers to adaptive, ai-integrated platforms. key contributions include: 1 treating loadable kernel modules lkms as ai-oriented computation units for fast sensory and cognitive processing in kernel space; 2 expanding the linux kernel into an ai-native environment with built-in deep learning inference, floating-point acceleration, and real-time adaptive scheduling for efficient ml workloads; and 3 introducing a neurosymbolic kernel design leveraging category theory and homotopy type theory to unify symbolic reasoning and differentiable logic within os internals. together, these approaches enable operating systems to proactively anticipate and adapt to the cognitive needs of autonomous intelligent applications.",
    "authors": [
      "Rajpreet Singh",
      "Vidhi Kothari"
    ],
    "author_count": 2,
    "categories": [
      "cs.OS",
      "cs.AI"
    ],
    "primary_category": "cs.OS",
    "keywords": [
      "kernel",
      "intelligent",
      "autonomous",
      "systems",
      "real-time",
      "cognitive",
      "theory",
      "composable",
      "architectures",
      "intelligence"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00604v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00604v1"
  },
  {
    "id": "2507.22645v1",
    "title": "From Tracepoints to Timeliness: A Semi-Markov Framework for Predictive Runtime Analysis",
    "abstract": "Detecting and resolving violations of temporal constraints in real-time\nsystems is both, time-consuming and resource-intensive, particularly in complex\nsoftware environments. Measurement-based approaches are widely used during\ndevelopment, but often are unable to deliver reliable predictions with limited\ndata. This paper presents a hybrid method for worst-case execution time\nestimation, combining lightweight runtime tracing with probabilistic modelling.\nTimestamped system events are used to construct a semi-Markov chain, where\ntransitions represent empirically observed timing between events. Execution\nduration is interpreted as time-to-absorption in the semi-Markov chain,\nenabling worst-case execution time estimation with fewer assumptions and\nreduced overhead. Empirical results from real-time Linux systems indicate that\nthe method captures both regular and extreme timing behaviours accurately, even\nfrom short observation periods. The model supports holistic, low-intrusion\nanalysis across system layers and remains interpretable and adaptable for\npractical use.",
    "clean_title": "from tracepoints to timeliness: a semi-markov framework for predictive runtime analysis",
    "clean_abstract": "detecting and resolving violations of temporal constraints in real-time systems is both, time-consuming and resource-intensive, particularly in complex software environments. measurement-based approaches are widely used during development, but often are unable to deliver reliable predictions with limited data. this paper presents a hybrid method for worst-case execution time estimation, combining lightweight runtime tracing with probabilistic modelling. timestamped system events are used to construct a semi-markov chain, where transitions represent empirically observed timing between events. execution duration is interpreted as time-to-absorption in the semi-markov chain, enabling worst-case execution time estimation with fewer assumptions and reduced overhead. empirical results from real-time linux systems indicate that the method captures both regular and extreme timing behaviours accurately, even from short observation periods. the model supports holistic, low-intrusion analysis across system layers and remains interpretable and adaptable for practical use.",
    "authors": [
      "Benno Bielmeier",
      "Ralf Ramsauer",
      "Takahiro Yoshida",
      "Wolfgang Mauerer"
    ],
    "author_count": 4,
    "categories": [
      "cs.OS"
    ],
    "primary_category": "cs.OS",
    "keywords": [
      "from",
      "semi-markov",
      "execution",
      "runtime",
      "analysis",
      "real-time",
      "systems",
      "worst-case",
      "time",
      "system"
    ],
    "published_date": "2025-07-30",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.22645v1",
    "pdf_url": "http://arxiv.org/pdf/2507.22645v1"
  },
  {
    "id": "2507.21248v1",
    "title": "Locked In, Leaked Out: Measuring Isolation via Kernel Locks",
    "abstract": "Isolation is a critical property for shared infrastructure to limit exposure\nand interference among simultaneous running workloads. Cloud providers use\ndifferent isolation mechanisms such as full Virtual Machines, microVMs, Linux\ncontainers, secure containers, etc., to confine workloads running in a\nmulti-tenant environment.\n  We propose a novel way to understand and measure performance interference and\nisolation at the system software layer that occurs due to shared access to data\nstructures. We observe that interference takes place through shared structures,\nsuch as a kernel-level data structure, and that operating systems must\nsynchronize access to these structures for safety. By measuring the level of\nsynchronization between workloads, we can measure their ability to interfere\nand thus the amount of isolation the platform provides\n  We demonstrate our method for measuring isolation by measuring the accesses\nto locks acquired in common across multiple workloads which indicates the\namount of sharing through kernel data structures and hence the\ninterference/isolation between two workloads. Furthermore, we identify the\nisolation properties of different kernel structures under different workloads\nand find that the file system journal and kernel page allocator are the most\ncommon sources of interference.",
    "clean_title": "locked in, leaked out: measuring isolation via kernel locks",
    "clean_abstract": "isolation is a critical property for shared infrastructure to limit exposure and interference among simultaneous running workloads. cloud providers use different isolation mechanisms such as full virtual machines, microvms, linux containers, secure containers, etc., to confine workloads running in a multi-tenant environment. we propose a novel way to understand and measure performance interference and isolation at the system software layer that occurs due to shared access to data structures. we observe that interference takes place through shared structures, such as a kernel-level data structure, and that operating systems must synchronize access to these structures for safety. by measuring the level of synchronization between workloads, we can measure their ability to interfere and thus the amount of isolation the platform provides we demonstrate our method for measuring isolation by measuring the accesses to locks acquired in common across multiple workloads which indicates the amount of sharing through kernel data structures and hence the interferenceisolation between two workloads. furthermore, we identify the isolation properties of different kernel structures under different workloads and find that the file system journal and kernel page allocator are the most common sources of interference.",
    "authors": [
      "Anjali",
      "Michael M. Swift"
    ],
    "author_count": 2,
    "categories": [
      "cs.OS",
      "D.4.0; C.4"
    ],
    "primary_category": "cs.OS",
    "keywords": [
      "isolation",
      "measuring",
      "kernel",
      "shared",
      "interference",
      "different",
      "workloads",
      "data",
      "structures",
      "locks"
    ],
    "published_date": "2025-07-28",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.21248v1",
    "pdf_url": "http://arxiv.org/pdf/2507.21248v1"
  },
  {
    "id": "2507.19212v1",
    "title": "Towards System-Level Quantum-Accelerator Integration",
    "abstract": "Quantum computers are often treated as experimental add-ons that are loosely\ncoupled to classical infrastructure through high-level interpreted languages\nand cloud-like orchestration. However, future deployments in both,\nhigh-performance computing (HPC) and embedded environments, will demand tighter\nintegration for lower latencies, stronger determinism, and architectural\nconsistency, as well as to implement error correction and other tasks that\nrequire tight quantum-classical interaction as generically as possible.\n  We propose a vertically integrated quantum systems architecture that treats\nquantum accelerators and processing units as peripheral system components. A\ncentral element is the Quantum Abstraction Layer (QAL) at operating system\nkernel level. It aims at real-time, low-latency, and high-throughput\ninteraction between quantum and classical resources, as well as robust\nlow-level quantum operations scheduling and generic resource management. It can\nserve as blueprint for orchestration of low-level computational components\n\"around\" a QPU (and inside a quantum computer), and across different\nmodalities.\n  We present first results towards such an integrated architecture, including a\nvirtual QPU model based on QEMU. The architecture is validated through\nfunctional emulation on three base architectures (x86_64, ARM64, and RISC-V),\nand timing-accurate FPGA-based simulations. This allows for a realistic\nevaluation of hybrid system performance and quantum advantage scenarios. Our\nwork lays the ground for a system-level co-design methodology tailored for the\nnext generation of quantum-classical computing.",
    "clean_title": "towards system-level quantum-accelerator integration",
    "clean_abstract": "quantum computers are often treated as experimental add-ons that are loosely coupled to classical infrastructure through high-level interpreted languages and cloud-like orchestration. however, future deployments in both, high-performance computing hpc and embedded environments, will demand tighter integration for lower latencies, stronger determinism, and architectural consistency, as well as to implement error correction and other tasks that require tight quantum-classical interaction as generically as possible. we propose a vertically integrated quantum systems architecture that treats quantum accelerators and processing units as peripheral system components. a central element is the quantum abstraction layer qal at operating system kernel level. it aims at real-time, low-latency, and high-throughput interaction between quantum and classical resources, as well as robust low-level quantum operations scheduling and generic resource management. it can serve as blueprint for orchestration of low-level computational components around a qpu and inside a quantum computer, and across different modalities. we present first results towards such an integrated architecture, including a virtual qpu model based on qemu. the architecture is validated through functional emulation on three base architectures x86_64, arm64, and risc-v, and timing-accurate fpga-based simulations. this allows for a realistic evaluation of hybrid system performance and quantum advantage scenarios. our work lays the ground for a system-level co-design methodology tailored for the next generation of quantum-classical computing.",
    "authors": [
      "Ralf Ramsauer",
      "Wolfgang Mauerer"
    ],
    "author_count": 2,
    "categories": [
      "quant-ph",
      "cs.OS"
    ],
    "primary_category": "quant-ph",
    "keywords": [
      "quantum",
      "system",
      "towards",
      "system-level",
      "integration",
      "classical",
      "through",
      "well",
      "quantum-classical",
      "interaction"
    ],
    "published_date": "2025-07-25",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.19212v1",
    "pdf_url": "http://arxiv.org/pdf/2507.19212v1"
  },
  {
    "id": "2507.18559v1",
    "title": "Optimizing Tree-structure Indexes for CXL-based Heterogeneous Memory with SINLK",
    "abstract": "On heterogeneous memory (HM) where fast memory (i.e., CPU-attached DRAM) and\nslow memory (e.g., remote NUMA memory, RDMA-connected memory, Persistent Memory\n(PM)) coexist, optimizing the placement of tree-structure indexes (e.g.,\nB+tree) is crucial to achieving high performance while enjoying memory\nexpansion. Nowadays, CXL-based heterogeneous memory (CXL-HM) is emerging due to\nits high efficiency and memory semantics. Prior tree-structure index placement\nschemes for HM cannot effectively boost performance on CXL-HM, as they fail to\nadapt to the changes in hardware characteristics and semantics. Additionally,\nexisting CXL-HM page-level data placement schemes are not efficient for\ntree-structure indexes due to the granularity mismatch between the tree nodes\nand the page.\n  In this paper, we argue for a CXL native, tree-structure aware data placement\nscheme to optimize tree-structure indexes on CXL-HM. Our key insight is that\nthe placement of tree-structure indexes on CXL-HM should match the tree's\ninherent characteristics with CXL-HM features. We present SINLK, a\ntree-structure aware, node-grained data placement scheme for tree-structure\nindexes on CXL-HM. With SINLK, developers can easily adapt existing\ntree-structure indexes to CXL-HM. We have integrated the B+tree and radix tree\nwith SINLK to demonstrate its effectiveness. Evaluations show that SINLK\nimproves throughput by up to 71% and reduces P99 latency by up to 81% compared\nwith state-of-the-art data placement schemes (e.g., MEMTIS) and HM-optimized\ntree-structure indexes in YCSB and real-world workloads.",
    "clean_title": "optimizing tree-structure indexes for cxl-based heterogeneous memory with sinlk",
    "clean_abstract": "on heterogeneous memory hm where fast memory i.e., cpu-attached dram and slow memory e.g., remote numa memory, rdma-connected memory, persistent memory pm coexist, optimizing the placement of tree-structure indexes e.g., btree is crucial to achieving high performance while enjoying memory expansion. nowadays, cxl-based heterogeneous memory cxl-hm is emerging due to its high efficiency and memory semantics. prior tree-structure index placement schemes for hm cannot effectively boost performance on cxl-hm, as they fail to adapt to the changes in hardware characteristics and semantics. additionally, existing cxl-hm page-level data placement schemes are not efficient for tree-structure indexes due to the granularity mismatch between the tree nodes and the page. in this paper, we argue for a cxl native, tree-structure aware data placement scheme to optimize tree-structure indexes on cxl-hm. our key insight is that the placement of tree-structure indexes on cxl-hm should match the trees inherent characteristics with cxl-hm features. we present sinlk, a tree-structure aware, node-grained data placement scheme for tree-structure indexes on cxl-hm. with sinlk, developers can easily adapt existing tree-structure indexes to cxl-hm. we have integrated the btree and radix tree with sinlk to demonstrate its effectiveness. evaluations show that sinlk improves throughput by up to 71 and reduces p99 latency by up to 81 compared with state-of-the-art data placement schemes e.g., memtis and hm-optimized tree-structure indexes in ycsb and real-world workloads.",
    "authors": [
      "Haoru Zhao",
      "Mingkai Dong",
      "Fangnuo Wu",
      "Haibo Chen"
    ],
    "author_count": 4,
    "categories": [
      "cs.OS"
    ],
    "primary_category": "cs.OS",
    "keywords": [
      "tree-structure",
      "indexes",
      "memory",
      "placement",
      "cxl-hm",
      "data",
      "heterogeneous",
      "sinlk",
      "e.g.,",
      "schemes"
    ],
    "published_date": "2025-07-24",
    "year": 2025,
    "month": 7,
    "url": "http://arxiv.org/abs/2507.18559v1",
    "pdf_url": "http://arxiv.org/pdf/2507.18559v1"
  },
  {
    "id": "2508.00772v1",
    "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement",
    "abstract": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.",
    "clean_title": "from code to career: assessing competitive programmers for industry placement",
    "clean_abstract": "in todays fast-paced tech industry, there is a growing need for tools that evaluate a programmers job readiness based on their coding performance. this study focuses on predicting the potential of codeforces users to secure various levels of software engineering jobs. the primary objective is to analyze how a users competitive programming activity correlates with their chances of obtaining positions, ranging from entry-level roles to jobs at major tech companies. we collect user data using the codeforces api, process key performance metrics, and build a prediction model using a random forest classifier. the model categorizes users into four levels of employability, ranging from those needing further development to those ready for top-tier tech jobs. the system is implemented using flask and deployed on render for real-time predictions. our evaluation demonstrates that the approach effectively distinguishes between different skill levels based on coding proficiency and participation. this work lays a foundation for the use of machine learning in career assessment and could be extended to predict job readiness in broader technical fields.",
    "authors": [
      "Md Imranur Rahman Akib",
      "Fathima Binthe Muhammed",
      "Umit Saha",
      "Md Fazlul Karim Patwary",
      "Mehrin Anannya",
      "Md Alomgeer Hussein",
      "Md Biplob Hosen"
    ],
    "author_count": 7,
    "categories": [
      "cs.SE",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "keywords": [
      "from",
      "tech",
      "users",
      "levels",
      "competitive",
      "programmers",
      "job",
      "readiness",
      "based",
      "their"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00772v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00772v1"
  },
  {
    "id": "2508.00534v1",
    "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations",
    "abstract": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.",
    "clean_title": "towards a unified framework for programming paradigms: a systematic review of classification formalisms and methodological foundations",
    "clean_abstract": "the rise of multi-paradigm languages challenges traditional classification methods, leading to practical software engineering issues like interoperability defects. this systematic literature review slr maps the formal foundations of programming paradigms. our objective is twofold: 1 to assess the state of the art of classification formalisms and their limitations, and 2 to identify the conceptual primitives and mathematical frameworks for a more powerful, reconstructive approach. based on a synthesis of 74 primary studies, we find that existing taxonomies lack conceptual granularity, a unified formal basis, and struggle with hybrid languages. in response, our analysis reveals a strong convergence toward a compositional reconstruction of paradigms. this approach identifies a minimal set of orthogonal, atomic primitives and leverages mathematical frameworks, predominantly type theory, category theory and unifying theories of programming utp, to formally guarantee their compositional properties. we conclude that the literature reflects a significant intellectual shift away from classification towards these promising formal, reconstructive frameworks. this review provides a map of this evolution and proposes a research agenda for their unification.",
    "authors": [
      "Mikel Vandeloise"
    ],
    "author_count": 1,
    "categories": [
      "cs.PL",
      "cs.CL",
      "D.3.2; F.3.2; D.3.1"
    ],
    "primary_category": "cs.PL",
    "keywords": [
      "classification",
      "programming",
      "review",
      "their",
      "towards",
      "unified",
      "systematic",
      "formalisms",
      "foundations",
      "literature"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00534v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00534v1"
  },
  {
    "id": "2508.00508v1",
    "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis",
    "abstract": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.",
    "clean_title": "desyan: a platform for seamless value-flow and symbolic analysis",
    "clean_abstract": "over the past two decades, two different types of static analyses have emerged as dominant paradigms both in academia and industry: value-flow analysis e.g., data-flow analysis or points-to analysis and symbolic analysis e.g., symbolic execution. despite their individual successes in numerous application fields, the two approaches have remained largely separate; an artifact of the simple reality that there is no broadly adopted unifying platform for effortless and efficient integration of symbolic techniques with high-performance data-flow reasoning. to bridge this gap, we introduce desyan: a platform for writing program analyses with seamless integration of value-flow and symbolic reasoning. desyan expands a production-ready datalog fixpoint engine souffle with full-fledged smt solving invoking industry-leading smt engines. desyan provides constructs for automatically and efficiently! handling typical patterns that come up in program analysis. at the same time, the integration is agnostic with respect to the solving technology, and supports datalog-native symbolic reasoning, via a bottom-up algebraic reasoning module. the result is an engine that allows blending different kinds of reasoning, as needed for the underlying analysis. for value-flow analysis, the engine is the best-in-class datalog evaluator often by a factor of over 20x in execution time; for applications that require full smt e.g., a concolic execution engine or other symbolic evaluator that needs to solve arbitrarily complex conditions, the engine is leveraging the leading smt solvers; for lightweight symbolic evaluation e.g., solving simple conditionals in the context of a path-sensitive analysis, the engine can use datalog-native symbolic reasoning, achieving large speedups often of over 2x compared to eagerly appealing to an smt solver.",
    "authors": [
      "Panagiotis Diamantakis",
      "Thanassis Avgerinos",
      "Yannis Smaragdakis"
    ],
    "author_count": 3,
    "categories": [
      "cs.SE",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "keywords": [
      "symbolic",
      "engine",
      "analysis",
      "smt",
      "value-flow",
      "e.g.,",
      "platform",
      "over",
      "two",
      "integration"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00508v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00508v1"
  },
  {
    "id": "2508.00482v1",
    "title": "Semantic Subtyping for Maps in Erlang",
    "abstract": "In this paper we will construct a set-theoretic model of types featuring type\nvariables, base types, set-theoretic types and map types. Syntax of map types\nspans all the map types available in Erlang. The model of types is used to\ndefine a semantic subtyping relation based on set containment. The novelty of\nthis work is the definition of subtyping over parameteric map types.",
    "clean_title": "semantic subtyping for maps in erlang",
    "clean_abstract": "in this paper we will construct a set-theoretic model of types featuring type variables, base types, set-theoretic types and map types. syntax of map types spans all the map types available in erlang. the model of types is used to define a semantic subtyping relation based on set containment. the novelty of this work is the definition of subtyping over parameteric map types.",
    "authors": [
      "Erdem Yildirim",
      "Albert Schimpf",
      "Stefan Wehr",
      "Annette Bieniusa"
    ],
    "author_count": 4,
    "categories": [
      "cs.PL"
    ],
    "primary_category": "cs.PL",
    "keywords": [
      "types",
      "map",
      "subtyping",
      "semantic",
      "set-theoretic",
      "model",
      "types.",
      "maps",
      "erlang",
      "construct"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00482v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00482v1"
  },
  {
    "id": "2508.00422v1",
    "title": "Automated Type Annotation in Python Using Large Language Models",
    "abstract": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby",
    "clean_title": "automated type annotation in python using large language models",
    "clean_abstract": "type annotations in python enhance maintainability and error detection. however, generating these annotations manually is error prone and requires extra effort. traditional automation approaches like static analysis, machine learning, and deep learning struggle with limited type vocabularies, behavioral over approximation, and reliance on large labeled datasets. in this work, we explore the use of llms for generating type annotations in python. we develop a generate check repair pipeline: the llm proposes annotations guided by a concrete syntax tree representation, a static type checker mypy verifies them, and any errors are fed back for iterative refinement. we evaluate four llm variants: gpt 4omini, gpt 4.1mini general-purpose, and o3mini, o4mini reasoning optimized, on 6000 code snippets from the manytypes4py benchmark. we first measure the proportion of code snippets annotated by llms for which mypy reported no errors i.e., consistent results: gpt 4omini achieved consistency on 65.9 of cases 34.1 inconsistent, while gpt 4.1mini, o3mini, and o4mini each reached approximately 88.6 consistency around 11.4 failures. to measure annotation quality, we then compute exact-match and base-type match accuracies over all 6000 snippets: gpt 4.1mini and o3mini perform the best, achieving up to 70.5 exact match and 79.1 base type accuracy, requiring under one repair iteration on average. our results demonstrate that general-purpose and reasoning optimized llms, without any task specific fine tuning or additional training can be effective in generating consistent type annotations.they perform competitively with traditional deep learning techniques which require large labeled dataset for training. while our work focuses on python, the pipeline can be extended to other optionally typed imperative languages like ruby",
    "authors": [
      "Varun Bharti",
      "Shashwat Jha",
      "Dhruv Kumar",
      "Pankaj Jalote"
    ],
    "author_count": 4,
    "categories": [
      "cs.PL",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "keywords": [
      "type",
      "gpt",
      "annotations",
      "large",
      "generating",
      "annotation",
      "python",
      "error",
      "traditional",
      "like"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00422v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00422v1"
  },
  {
    "id": "2508.00419v1",
    "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers",
    "abstract": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.",
    "clean_title": "loop invariant generation: a hybrid framework of reasoning optimised llms and smt solvers",
    "clean_abstract": "loop invariants are essential for proving the correctness of programs with loops. developing loop invariants is challenging, and fully automatic synthesis cannot be guaranteed for arbitrary programs. some approaches have been proposed to synthesize loop invariants using symbolic techniques and more recently using neural approaches. these approaches are able to correctly synthesize loop invariants only for subsets of standard benchmarks. in this work, we investigate whether modern, reasoning-optimized large language models can do better. we integrate openais o1, o1-mini, and o3-mini into a tightly coupled generate-and-check pipeline with the z3 smt solver, using solver counterexamples to iteratively guide invariant refinement. we use code2inv benchmark, which provides c programs along with their formal preconditions and postconditions. on this benchmark of 133 tasks, our framework achieves 100 coverage 133 out of 133, outperforming the previous best of 107 out of 133, while requiring only 1-2 model proposals per instance and 14-55 seconds of wall-clock time. these results demonstrate that llms possess latent logical reasoning capabilities which can help automate loop invariant synthesis. while our experiments target c-specific programs, this approach should be generalizable to other imperative languages.",
    "authors": [
      "Varun Bharti",
      "Shashwat Jha",
      "Dhruv Kumar",
      "Pankaj Jalote"
    ],
    "author_count": 4,
    "categories": [
      "cs.LO",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.LO",
    "keywords": [
      "loop",
      "invariants",
      "invariant",
      "framework",
      "reasoning",
      "llms",
      "smt",
      "programs",
      "approaches",
      "synthesize"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00419v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00419v1"
  },
  {
    "id": "2508.00772v1",
    "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement",
    "abstract": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.",
    "clean_title": "from code to career: assessing competitive programmers for industry placement",
    "clean_abstract": "in todays fast-paced tech industry, there is a growing need for tools that evaluate a programmers job readiness based on their coding performance. this study focuses on predicting the potential of codeforces users to secure various levels of software engineering jobs. the primary objective is to analyze how a users competitive programming activity correlates with their chances of obtaining positions, ranging from entry-level roles to jobs at major tech companies. we collect user data using the codeforces api, process key performance metrics, and build a prediction model using a random forest classifier. the model categorizes users into four levels of employability, ranging from those needing further development to those ready for top-tier tech jobs. the system is implemented using flask and deployed on render for real-time predictions. our evaluation demonstrates that the approach effectively distinguishes between different skill levels based on coding proficiency and participation. this work lays a foundation for the use of machine learning in career assessment and could be extended to predict job readiness in broader technical fields.",
    "authors": [
      "Md Imranur Rahman Akib",
      "Fathima Binthe Muhammed",
      "Umit Saha",
      "Md Fazlul Karim Patwary",
      "Mehrin Anannya",
      "Md Alomgeer Hussein",
      "Md Biplob Hosen"
    ],
    "author_count": 7,
    "categories": [
      "cs.SE",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "keywords": [
      "from",
      "tech",
      "users",
      "levels",
      "competitive",
      "programmers",
      "job",
      "readiness",
      "based",
      "their"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00772v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00772v1"
  },
  {
    "id": "2508.00749v1",
    "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures",
    "abstract": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.",
    "clean_title": "dynamic symbolic execution for semantic difference analysis of component and connector architectures",
    "clean_abstract": "in the context of model-driven development, ensuring the correctness and consistency of evolving models is paramount. this paper investigates the application of dynamic symbolic execution dse for semantic difference analysis of component-and-connector architectures, specifically utilizing montiarc models. we have enhanced the existing montiarc-to-java generator to gather both symbolic and concrete execution data at runtime, encompassing transition conditions, visited states, and internal variables of automata. this data facilitates the identification of significant execution traces that provide critical insights into system behavior. we evaluate various execution strategies based on the criteria of runtime efficiency, minimality, and completeness, establishing a framework for assessing the applicability of dse in semantic difference analysis. our findings indicate that while dse shows promise for analyzing component and connector architectures, scalability remains a primary limitation, suggesting further research is needed to enhance its practical utility in larger systems.",
    "authors": [
      "Johanna Grahl",
      "Bernhard Rumpe",
      "Max Stachon",
      "Sebastian Stber"
    ],
    "author_count": 4,
    "categories": [
      "cs.SE",
      "cs.FL",
      "cs.SC",
      "68N30",
      "D.2.4"
    ],
    "primary_category": "cs.SE",
    "keywords": [
      "execution",
      "symbolic",
      "semantic",
      "difference",
      "dse",
      "dynamic",
      "analysis",
      "component",
      "connector",
      "architectures,"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00749v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00749v1"
  },
  {
    "id": "2508.00738v1",
    "title": "Tool-Assisted Conformance Checking to Reference Process Models",
    "abstract": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.",
    "clean_title": "tool-assisted conformance checking to reference process models",
    "clean_abstract": "reference models convey best practices and standards. the reference frameworks necessitate conformance checks to ensure adherence to established guidelines and principles, which is crucial for maintaining quality and consistency in various processes. this paper explores automated conformance checks for concrete process models against reference models using causal dependency analysis of tasks and events. existing notions of conformance checking for process models focus on verifying process execution traces and lack the expressiveness and automation needed for semantic model comparison, leaving this question unresolved. we integrate our approach into a broader semantic framework for defining reference model conformance. we outline an algorithm for reference process model conformance checking, evaluate it through a case study, and discuss its strengths and limitations. our research provides a tool-assisted solution enhancing accuracy and flexibility in process model conformance verification.",
    "authors": [
      "Bernhard Rumpe",
      "Max Stachon",
      "Sebastian Stber",
      "Valdes Voufo"
    ],
    "author_count": 4,
    "categories": [
      "cs.SE",
      "cs.FL",
      "68N30",
      "D.2.4"
    ],
    "primary_category": "cs.SE",
    "keywords": [
      "conformance",
      "reference",
      "process",
      "models",
      "model",
      "tool-assisted",
      "checking",
      "checks",
      "semantic",
      "convey"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00738v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00738v1"
  },
  {
    "id": "2508.00700v1",
    "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?",
    "abstract": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.",
    "clean_title": "is llm-generated code more maintainable  reliable than human-written code?",
    "clean_abstract": "background: the rise of large language models llms in software development has opened new possibilities for code generation. despite the widespread use of this technology, it remains unclear how well llms generate code solutions in terms of software quality and how they compare to human-written code. aims: this study compares the internal quality attributes of llm-generated and human-written code. method: our empirical study integrates datasets of coding tasks, three llm configurations zero-shot, few-shot, and fine-tuning, and sonarqube to assess software quality. the dataset comprises python code solutions across three difficulty levels: introductory, interview, and competition. we analyzed key code quality metrics, including maintainability and reliability, and the estimated effort required to resolve code issues. results: our analysis shows that llm-generated code has fewer bugs and requires less effort to fix them overall. interestingly, fine-tuned models reduced the prevalence of high-severity issues, such as blocker and critical bugs, and shifted them to lower-severity categories, but decreased the models performance. in competition-level problems, the llm solutions sometimes introduce structural issues that are not present in human-written code. conclusion: our findings provide valuable insights into the quality of llm-generated code; however, the introduction of critical issues in more complex scenarios highlights the need for a systematic evaluation and validation of llm solutions. our work deepens the understanding of the strengths and limitations of llms for code generation.",
    "authors": [
      "Alfred Santa Molison",
      "Marcia Moraes",
      "Glaucia Melo",
      "Fabio Santos",
      "Wesley K. G. Assuncao"
    ],
    "author_count": 5,
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE",
    "keywords": [
      "code",
      "llm-generated",
      "human-written",
      "quality",
      "models",
      "llms",
      "software",
      "solutions",
      "code.",
      "llm"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00700v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00700v1"
  },
  {
    "id": "2508.00682v1",
    "title": "Unveiling Dynamic Binary Instrumentation Techniques",
    "abstract": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances.",
    "clean_title": "unveiling dynamic binary instrumentation techniques",
    "clean_abstract": "dynamic binary instrumentation dbi is the set of techniques that enable instrumentation of programs at run-time, making it possible to monitor and modify the execution of compiled binaries or entire systems. dbi is used for countless security applications and analyses, and is extensively used across many fields in both industry and academia. over the years, several dbi approaches have been proposed based on different technologies and implementing diverse techniques. every solution tries to overcome certain limitations, but they sometimes bring other shortcomings. some are specialized for one particular domain or task, while others have a wider scope. in this paper, we shed light into the labyrinth of dbi, bringing together process-level and whole-system approaches. we depict their building blocks and analyze the underlying instrumentation techniques, comparing their ability to instrument different primitives and run-time events. then, we evaluate their performance when implementing each primitive, and highlight relevant observations. our results show that no single technique is better than the rest in all circumstances.",
    "authors": [
      "Oscar Llorente-Vazquez",
      "Xabier Ugarte-Pedrero",
      "Igor Santos-Grueiro",
      "Pablo Garcia Bringas"
    ],
    "author_count": 4,
    "categories": [
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "keywords": [
      "instrumentation",
      "dbi",
      "their",
      "dynamic",
      "binary",
      "techniques",
      "different",
      "implementing",
      "unveiling",
      "set"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00682v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00682v1"
  },
  {
    "id": "2508.00654v1",
    "title": "LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources",
    "abstract": "In the interdisciplinary field of microscopy research, managing and\nintegrating large volumes of data stored across disparate platforms remains a\nmajor challenge. Data types such as bioimages, experimental records, and\nspectral information are often maintained in separate repositories, each\nfollowing different management standards. However, linking these data sources\nacross the research lifecycle is essential to align with the FAIR principles of\ndata management: Findability, Accessibility, Interoperability, and Reusability.\nDespite this need, there is a notable lack of tools capable of effectively\nintegrating and linking data from heterogeneous sources. To address this gap,\nwe present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based\nplatform designed to create and manage links between distributed data systems.\nLEO was initially developed to link objects between Electronic Lab Notebooks\n(ELNs) and OMERO, but its functionality has since been extended through a\nplugin-based architecture, allowing the integration of additional data sources.\nThis extensibility makes LEO a scalable and flexible solution for a wide range\nof microscopy research workflows.",
    "clean_title": "leo: an open-source platform for linking omero with lab notebooks and heterogeneous metadata sources",
    "clean_abstract": "in the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. however, linking these data sources across the research lifecycle is essential to align with the fair principles of data management: findability, accessibility, interoperability, and reusability. despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. to address this gap, we present leo linking electronic lab notebooks with omero, a web-based platform designed to create and manage links between distributed data systems. leo was initially developed to link objects between electronic lab notebooks elns and omero, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. this extensibility makes leo a scalable and flexible solution for a wide range of microscopy research workflows.",
    "authors": [
      "Rodrigo Escobar Daz Guerrero",
      "Jamile Mohammad Jafari",
      "Tobias Meyer-Zedler",
      "Michael Schmitt",
      "Juergen Popp",
      "Thomas Bocklitz"
    ],
    "author_count": 6,
    "categories": [
      "cs.CE",
      "cs.SE"
    ],
    "primary_category": "cs.CE",
    "keywords": [
      "data",
      "linking",
      "lab",
      "notebooks",
      "leo",
      "platform",
      "heterogeneous",
      "sources",
      "microscopy",
      "integrating"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00654v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00654v1"
  },
  {
    "id": "2508.00804v1",
    "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models",
    "abstract": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.",
    "clean_title": "online fine-tuning of carbon emission predictions using real-time recurrent learning for state space models",
    "clean_abstract": "this paper introduces a new approach for fine-tuning the predictions of structured state space models ssms at inference time using real-time recurrent learning. while ssms are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. our method enables online adaptation by continuously updating model parameters in response to incoming data. we evaluate our approach for linear-recurrent-unit ssms using a small carbon emission dataset collected from embedded automotive hardware. experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.",
    "authors": [
      "Julian Lemmel",
      "Manuel Kranzl",
      "Adam Lamine",
      "Philipp Neubauer",
      "Radu Grosu",
      "Sophie Neubauer"
    ],
    "author_count": 6,
    "categories": [
      "cs.CE",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CE",
    "keywords": [
      "online",
      "ssms",
      "fine-tuning",
      "carbon",
      "emission",
      "predictions",
      "real-time",
      "recurrent",
      "state",
      "space"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00804v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00804v1"
  },
  {
    "id": "2508.00775v1",
    "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms",
    "abstract": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.",
    "clean_title": "learning to optimize with guarantees: a complete characterization of linearly convergent algorithms",
    "clean_abstract": "in high-stakes engineering applications, optimization algorithms must come with provable worst-case guarantees over a mathematically defined class of problems. designing for the worst case, however, inevitably sacrifices performance on the specific problem instances that often occur in practice. we address the problem of augmenting a given linearly convergent algorithm to improve its average-case performance on a restricted set of target problems - for example, tailoring an off-the-shelf solver for model predictive control mpc for an application to a specific dynamical system - while preserving its worst-case guarantees across the entire problem class. toward this goal, we characterize the class of algorithms that achieve linear convergence for classes of nonsmooth composite optimization problems. in particular, starting from a baseline linearly convergent algorithm, we derive all - and only - the modifications to its update rule that maintain its convergence properties. our results apply to augmenting legacy algorithms such as gradient descent for nonconvex, gradient-dominated functions; nesterovs accelerated method for strongly convex functions; and projected methods for optimization over polyhedral feasibility sets. we showcase effectiveness of the approach on solving optimization problems with tight iteration budgets in application to ill-conditioned systems of linear equations and mpc for linear systems.",
    "authors": [
      "Andrea Martin",
      "Ian R. Manchester",
      "Luca Furieri"
    ],
    "author_count": 3,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "keywords": [
      "algorithms",
      "optimization",
      "its",
      "linearly",
      "convergent",
      "problem",
      "linear",
      "worst-case",
      "guarantees",
      "over"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00775v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00775v1"
  },
  {
    "id": "2508.00724v1",
    "title": "Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems",
    "abstract": "The increasing demand for automation and flexibility drives the widespread\nadoption of heterogeneous automated guided vehicles (AGVs). This work intends\nto investigate a new scheduling problem in a material transportation system\nconsisting of attachable heterogeneous AGVs, namely carriers and shuttles. They\ncan flexibly attach to and detach from each other to cooperatively execute\ncomplex transportation tasks. While such collaboration enhances operational\nefficiency, the attachment-induced synchronization and interdependence render\nthe scheduling coupled and susceptible to deadlock. To tackle this challenge,\nPetri nets are introduced to model AGV schedules, well describing the\nconcurrent and sequential task execution and carrier-shuttle synchronization.\nBased on Petri net theory, a firing-driven decoding method is proposed, along\nwith deadlock detection and prevention strategies to ensure deadlock-free\nschedules. Furthermore, a Petri net-based metaheuristic is developed in an\nadaptive large neighborhood search framework and incorporates an effective\nacceleration method to enhance computational efficiency. Finally, numerical\nexperiments using real-world industrial data validate the effectiveness of the\nproposed algorithm against the scheduling policy applied in engineering\npractice, an exact solver, and four state-of-the-art metaheuristics. A\nsensitivity analysis is also conducted to provide managerial insights.",
    "clean_title": "petri net modeling and deadlock-free scheduling of attachable heterogeneous agv systems",
    "clean_abstract": "the increasing demand for automation and flexibility drives the widespread adoption of heterogeneous automated guided vehicles agvs. this work intends to investigate a new scheduling problem in a material transportation system consisting of attachable heterogeneous agvs, namely carriers and shuttles. they can flexibly attach to and detach from each other to cooperatively execute complex transportation tasks. while such collaboration enhances operational efficiency, the attachment-induced synchronization and interdependence render the scheduling coupled and susceptible to deadlock. to tackle this challenge, petri nets are introduced to model agv schedules, well describing the concurrent and sequential task execution and carrier-shuttle synchronization. based on petri net theory, a firing-driven decoding method is proposed, along with deadlock detection and prevention strategies to ensure deadlock-free schedules. furthermore, a petri net-based metaheuristic is developed in an adaptive large neighborhood search framework and incorporates an effective acceleration method to enhance computational efficiency. finally, numerical experiments using real-world industrial data validate the effectiveness of the proposed algorithm against the scheduling policy applied in engineering practice, an exact solver, and four state-of-the-art metaheuristics. a sensitivity analysis is also conducted to provide managerial insights.",
    "authors": [
      "Boyu Li",
      "Zhengchen Li",
      "Weimin Wu",
      "Mengchu Zhou"
    ],
    "author_count": 4,
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "keywords": [
      "petri",
      "scheduling",
      "heterogeneous",
      "net",
      "deadlock-free",
      "attachable",
      "agv",
      "transportation",
      "modeling",
      "systems"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00724v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00724v1"
  },
  {
    "id": "2508.00692v1",
    "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network",
    "abstract": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.",
    "clean_title": "wind power scenario generation based on the generalized dynamic factor model and generative adversarial network",
    "clean_abstract": "for conducting resource adequacy studies, we synthesize multiple long-term wind power scenarios of distributed wind farms simultaneously by using the spatio-temporal features: spatial and temporal correlation, waveforms, marginal and ramp rates distributions of waveform, power spectral densities, and statistical characteristics. generating the spatial correlation in scenarios requires the design of common factors for neighboring wind farms and antithetical factors for distant wind farms. the generalized dynamic factor model gdfm can extract the common factors through cross spectral density analysis, but it cannot closely imitate waveforms. the gan can synthesize plausible samples representing the temporal correlation by verifying samples through a fake sample discriminator. to combine the advantages of gdfm and gan, we use the gan to provide a filter that extracts dynamic factors with temporal information from the observation data, and we then apply this filter in the gdfm to represent both spatial and frequency correlations of plausible waveforms. numerical tests on the combination of gdfm and gan have demonstrated performance improvements over competing alternatives in synthesizing wind power scenarios from australia, better realizing plausible statistical characteristics of actual wind power compared to alternatives such as the gdfm with a filter synthesized from distributions of actual dynamic filters and the gan with direct synthesis without dynamic factors.",
    "authors": [
      "Young-ho Cho",
      "Hao Zhu",
      "Duehee Lee",
      "Ross Baldick"
    ],
    "author_count": 4,
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "keywords": [
      "wind",
      "power",
      "dynamic",
      "gdfm",
      "factors",
      "gan",
      "scenarios",
      "spatial",
      "temporal",
      "plausible"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00692v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00692v1"
  },
  {
    "id": "2508.00663v1",
    "title": "Organic Electrochemical Neurons: Nonlinear Tools for Complex Dynamics",
    "abstract": "Hybrid oscillator architectures that combine feedback oscillators with\nself-sustained negative resistance oscillators have emerged as a promising\nplatform for artificial neuron design. In this work, we introduce a modeling\nand analysis framework for amplifier-assisted organic electrochemical neurons,\nleveraging nonlinear dynamical systems theory. By formulating the system as\ncoupled differential equations describing membrane voltage and internal state\nvariables, we identify the conditions for self-sustained oscillations and\ncharacterize the resulting dynamics through nullclines, phase-space analysis,\nand bifurcation behavior, providing complementary insight to standard\ncircuit-theoretic arguments of the operation of oscillators. Our simplified yet\nrigorous model enables tractable analysis of circuits integrating classical\nfeedback components (e.g., operational amplifiers) with novel devices\nexhibiting negative differential resistance, such as organic electrochemical\ntransistors (OECT). This approach reveals the core mechanisms behind\noscillation generation, demonstrating the utility of dynamic systems theory in\nunderstanding and designing complex hybrid circuits. Beyond neuromorphic and\nbioelectronic applications, the proposed framework offers a generalizable\nfoundation for developing tunable, biologically inspired oscillatory systems in\nsensing, signal processing, and adaptive control.",
    "clean_title": "organic electrochemical neurons: nonlinear tools for complex dynamics",
    "clean_abstract": "hybrid oscillator architectures that combine feedback oscillators with self-sustained negative resistance oscillators have emerged as a promising platform for artificial neuron design. in this work, we introduce a modeling and analysis framework for amplifier-assisted organic electrochemical neurons, leveraging nonlinear dynamical systems theory. by formulating the system as coupled differential equations describing membrane voltage and internal state variables, we identify the conditions for self-sustained oscillations and characterize the resulting dynamics through nullclines, phase-space analysis, and bifurcation behavior, providing complementary insight to standard circuit-theoretic arguments of the operation of oscillators. our simplified yet rigorous model enables tractable analysis of circuits integrating classical feedback components e.g., operational amplifiers with novel devices exhibiting negative differential resistance, such as organic electrochemical transistors oect. this approach reveals the core mechanisms behind oscillation generation, demonstrating the utility of dynamic systems theory in understanding and designing complex hybrid circuits. beyond neuromorphic and bioelectronic applications, the proposed framework offers a generalizable foundation for developing tunable, biologically inspired oscillatory systems in sensing, signal processing, and adaptive control.",
    "authors": [
      "Gonzalo Rivera-Sierra",
      "Roberto Fenollosa",
      "Juan Bisquert"
    ],
    "author_count": 3,
    "categories": [
      "physics.chem-ph",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "physics.chem-ph",
    "keywords": [
      "organic",
      "electrochemical",
      "systems",
      "nonlinear",
      "complex",
      "dynamics",
      "hybrid",
      "feedback",
      "oscillators",
      "self-sustained"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00663v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00663v1"
  },
  {
    "id": "2508.00637v1",
    "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks",
    "abstract": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios.",
    "clean_title": "cyber-physical co-simulation of load frequency control under load-altering attacks",
    "clean_abstract": "integrating information and communications technology ict devices into the power grid brings many benefits. however, it also exposes the grid to new potential cyber threats. many control and protection mechanisms, such as load frequency control lfc, responsible for maintaining nominal frequency during load fluctuations and under frequency load shedding ufls disconnecting portion of the load during an emergency, are dependent on information exchange through the communication network. the recently emerging load altering attacks laas utilize a botnet of high-wattage devices to introduce load fluctuation. in their dynamic form dlaas, they manipulate the load in response to live grid frequency measurements for increased efficiency, posing a notable threat to grid stability. recognizing the importance of communication networks in power grid cyber security research, this paper presents an open-source co-simulation environment that models the power grid with the corresponding communication network, implementing grid protective mechanisms. this setup allows the comprehensive analysis of the attacks in concrete lfc and ufls scenarios.",
    "authors": [
      "Micha Forystek",
      "Andrew D. Syrmakesis",
      "Alkistis Kontou",
      "Panos Kotsampopoulos",
      "Nikos D. Hatziargyriou",
      "Charalambos Konstantinou"
    ],
    "author_count": 6,
    "categories": [
      "eess.SY",
      "cs.CR",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "keywords": [
      "load",
      "grid",
      "frequency",
      "control",
      "attacks",
      "power",
      "communication",
      "co-simulation",
      "under",
      "information"
    ],
    "published_date": "2025-08-01",
    "year": 2025,
    "month": 8,
    "url": "http://arxiv.org/abs/2508.00637v1",
    "pdf_url": "http://arxiv.org/pdf/2508.00637v1"
  }
]